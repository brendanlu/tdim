<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 14.5.0"/>
    <title>tred API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .pdoc-alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:1rem center;margin-bottom:1rem;}.pdoc .pdoc-alert > *:last-child{margin-bottom:0;}.pdoc .pdoc-alert-note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style><script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
    };
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
    /* Re-invoke MathJax when DOM content changes, for example during search. */
    document.addEventListener("DOMContentLoaded", () => {
        new MutationObserver(() => MathJax.typeset()).observe(
            document.querySelector("main.pdoc").parentNode,
            {childList: true}
        );
    })
</script>
<style>
    mjx-container {
        overflow-x: auto;
        overflow-y: hidden;
    }
</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>

            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>


            <h2>Submodules</h2>
            <ul>
                    <li><a href="tred/datasets.html">datasets</a></li>
            </ul>

            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="function" href="#tsvdm">tsvdm</a>
            </li>
            <li>
                    <a class="class" href="#TPCA">TPCA</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#TPCA.fit">fit</a>
                        </li>
                        <li>
                                <a class="function" href="#TPCA.transform">transform</a>
                        </li>
                        <li>
                                <a class="function" href="#TPCA.fit_transform">fit_transform</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="function" href="#display_tensor_facewise">display_tensor_facewise</a>
            </li>
            <li>
                    <a class="function" href="#generate_default_m_transform_pair">generate_default_m_transform_pair</a>
            </li>
            <li>
                    <a class="function" href="#generate_transform_pair_from_matrix">generate_transform_pair_from_matrix</a>
            </li>
            <li>
                    <a class="function" href="#generate_dctii_m_transform_pair">generate_dctii_m_transform_pair</a>
            </li>
            <li>
                    <a class="function" href="#generate_dstii_m_transform_pair">generate_dstii_m_transform_pair</a>
            </li>
            <li>
                    <a class="function" href="#facewise_product">facewise_product</a>
            </li>
            <li>
                    <a class="function" href="#m_product">m_product</a>
            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
tred    </h1>

                        <div class="docstring"><p>The <code><a href="">tred</a></code> module implements tensor decomposition methods such as tensor PCA 
and tensor SVD. Most of the functionality in this module can be regarded as 
dimensionality reduction strategies for order-3 datasets of shape n, p, t, 
where p &gt;&gt; n &gt; t.</p>
</div>

                        <input id="mod-tred-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-tred-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos"> 1</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos"> 2</span></a><span class="sd">The `tred` module implements tensor decomposition methods such as tensor PCA </span>
</span><span id="L-3"><a href="#L-3"><span class="linenos"> 3</span></a><span class="sd">and tensor SVD. Most of the functionality in this module can be regarded as </span>
</span><span id="L-4"><a href="#L-4"><span class="linenos"> 4</span></a><span class="sd">dimensionality reduction strategies for order-3 datasets of shape n, p, t, </span>
</span><span id="L-5"><a href="#L-5"><span class="linenos"> 5</span></a><span class="sd">where p &gt;&gt; n &gt; t. </span>
</span><span id="L-6"><a href="#L-6"><span class="linenos"> 6</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-7"><a href="#L-7"><span class="linenos"> 7</span></a><span class="n">__version__</span> <span class="o">=</span> <span class="s2">&quot;0.1.3&quot;</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos"> 8</span></a>
</span><span id="L-9"><a href="#L-9"><span class="linenos"> 9</span></a>
</span><span id="L-10"><a href="#L-10"><span class="linenos">10</span></a><span class="kn">from</span> <span class="nn">._m_transforms</span> <span class="kn">import</span> <span class="p">(</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos">11</span></a>    <span class="n">generate_default_m_transform_pair</span><span class="p">,</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos">12</span></a>    <span class="n">generate_transform_pair_from_matrix</span><span class="p">,</span>
</span><span id="L-13"><a href="#L-13"><span class="linenos">13</span></a>    <span class="n">generate_dstii_m_transform_pair</span><span class="p">,</span>
</span><span id="L-14"><a href="#L-14"><span class="linenos">14</span></a>    <span class="n">generate_dctii_m_transform_pair</span><span class="p">,</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos">15</span></a><span class="p">)</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos">16</span></a><span class="kn">from</span> <span class="nn">._tensor_ops</span> <span class="kn">import</span> <span class="n">facewise_product</span><span class="p">,</span> <span class="n">m_product</span><span class="p">,</span> <span class="n">tsvdm</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos">17</span></a><span class="kn">from</span> <span class="nn">._tensor_pca</span> <span class="kn">import</span> <span class="n">TPCA</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos">18</span></a><span class="kn">from</span> <span class="nn">._utils</span> <span class="kn">import</span> <span class="n">display_tensor_facewise</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos">19</span></a>
</span><span id="L-20"><a href="#L-20"><span class="linenos">20</span></a><span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos">21</span></a>    <span class="s2">&quot;tsvdm&quot;</span><span class="p">,</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos">22</span></a>    <span class="s2">&quot;TPCA&quot;</span><span class="p">,</span>
</span><span id="L-23"><a href="#L-23"><span class="linenos">23</span></a>    <span class="s2">&quot;display_tensor_facewise&quot;</span><span class="p">,</span>
</span><span id="L-24"><a href="#L-24"><span class="linenos">24</span></a>    <span class="s2">&quot;generate_default_m_transform_pair&quot;</span><span class="p">,</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos">25</span></a>    <span class="s2">&quot;generate_transform_pair_from_matrix&quot;</span><span class="p">,</span>
</span><span id="L-26"><a href="#L-26"><span class="linenos">26</span></a>    <span class="s2">&quot;generate_dctii_m_transform_pair&quot;</span><span class="p">,</span>
</span><span id="L-27"><a href="#L-27"><span class="linenos">27</span></a>    <span class="s2">&quot;generate_dstii_m_transform_pair&quot;</span><span class="p">,</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos">28</span></a>    <span class="s2">&quot;facewise_product&quot;</span><span class="p">,</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos">29</span></a>    <span class="s2">&quot;m_product&quot;</span><span class="p">,</span>
</span><span id="L-30"><a href="#L-30"><span class="linenos">30</span></a>    <span class="s2">&quot;datasets&quot;</span><span class="p">,</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos">31</span></a><span class="p">]</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos">32</span></a>
</span><span id="L-33"><a href="#L-33"><span class="linenos">33</span></a><span class="c1"># private - for testing</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos">34</span></a><span class="c1">###############################################################################</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos">35</span></a><span class="kn">from</span> <span class="nn">._tensor_ops</span> <span class="kn">import</span> <span class="n">_mode_1_unfold</span><span class="p">,</span> <span class="n">_mode_2_unfold</span><span class="p">,</span> <span class="n">_mode_3_unfold</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos">36</span></a><span class="kn">from</span> <span class="nn">._utils</span> <span class="kn">import</span> <span class="n">_singular_vals_tensor_to_mat</span><span class="p">,</span> <span class="n">_singular_vals_mat_to_tensor</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos">37</span></a>
</span><span id="L-38"><a href="#L-38"><span class="linenos">38</span></a>
</span><span id="L-39"><a href="#L-39"><span class="linenos">39</span></a><span class="c1"># documentation configurations</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos">40</span></a><span class="c1">###############################################################################</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos">41</span></a><span class="n">__docformat__</span> <span class="o">=</span> <span class="s2">&quot;numpy&quot;</span>
</span></pre></div>


            </section>
                <section id="tsvdm">
                            <input id="tsvdm-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">tsvdm</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">A</span>,</span><span class="param">	<span class="n">M</span><span class="o">=</span><span class="kc">None</span>,</span><span class="param">	<span class="n">Minv</span><span class="o">=</span><span class="kc">None</span>,</span><span class="param">	<span class="o">*</span>,</span><span class="param">	<span class="n">keep_hats</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">full_frontal_slices</span><span class="o">=</span><span class="kc">True</span>,</span><span class="param">	<span class="n">svals_matrix_form</span><span class="o">=</span><span class="kc">False</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="tsvdm-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#tsvdm"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="tsvdm-90"><a href="#tsvdm-90"><span class="linenos"> 90</span></a><span class="k">def</span> <span class="nf">tsvdm</span><span class="p">(</span>
</span><span id="tsvdm-91"><a href="#tsvdm-91"><span class="linenos"> 91</span></a>    <span class="n">A</span><span class="p">,</span>
</span><span id="tsvdm-92"><a href="#tsvdm-92"><span class="linenos"> 92</span></a>    <span class="n">M</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="tsvdm-93"><a href="#tsvdm-93"><span class="linenos"> 93</span></a>    <span class="n">Minv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="tsvdm-94"><a href="#tsvdm-94"><span class="linenos"> 94</span></a>    <span class="o">*</span><span class="p">,</span>
</span><span id="tsvdm-95"><a href="#tsvdm-95"><span class="linenos"> 95</span></a>    <span class="n">keep_hats</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="tsvdm-96"><a href="#tsvdm-96"><span class="linenos"> 96</span></a>    <span class="n">full_frontal_slices</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="tsvdm-97"><a href="#tsvdm-97"><span class="linenos"> 97</span></a>    <span class="n">svals_matrix_form</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="tsvdm-98"><a href="#tsvdm-98"><span class="linenos"> 98</span></a><span class="p">):</span>
</span><span id="tsvdm-99"><a href="#tsvdm-99"><span class="linenos"> 99</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Return the t-SVDM decomposition from Kilmer et al. (2021).</span>
</span><span id="tsvdm-100"><a href="#tsvdm-100"><span class="linenos">100</span></a>
</span><span id="tsvdm-101"><a href="#tsvdm-101"><span class="linenos">101</span></a><span class="sd">    This is a modified version, inspired by the implementation at</span>
</span><span id="tsvdm-102"><a href="#tsvdm-102"><span class="linenos">102</span></a><span class="sd">    https://github.com/UriaMorP/mprod_package</span>
</span><span id="tsvdm-103"><a href="#tsvdm-103"><span class="linenos">103</span></a>
</span><span id="tsvdm-104"><a href="#tsvdm-104"><span class="linenos">104</span></a><span class="sd">    *NOTE: For now, unlike some other implementations (Numpy, Scipy), we</span>
</span><span id="tsvdm-105"><a href="#tsvdm-105"><span class="linenos">105</span></a><span class="sd">    will return the tensor $V$ NOT $V^T$.*</span>
</span><span id="tsvdm-106"><a href="#tsvdm-106"><span class="linenos">106</span></a>
</span><span id="tsvdm-107"><a href="#tsvdm-107"><span class="linenos">107</span></a><span class="sd">    Parameters</span>
</span><span id="tsvdm-108"><a href="#tsvdm-108"><span class="linenos">108</span></a><span class="sd">    ----------</span>
</span><span id="tsvdm-109"><a href="#tsvdm-109"><span class="linenos">109</span></a><span class="sd">    A : ndarray, shape: (n, p, t)</span>
</span><span id="tsvdm-110"><a href="#tsvdm-110"><span class="linenos">110</span></a><span class="sd">        Data tensor</span>
</span><span id="tsvdm-111"><a href="#tsvdm-111"><span class="linenos">111</span></a>
</span><span id="tsvdm-112"><a href="#tsvdm-112"><span class="linenos">112</span></a><span class="sd">    M : Callable[[ndarray], ndarray] or None, default=None</span>
</span><span id="tsvdm-113"><a href="#tsvdm-113"><span class="linenos">113</span></a><span class="sd">        A function which expects an order-3 tensor as input, and returns the</span>
</span><span id="tsvdm-114"><a href="#tsvdm-114"><span class="linenos">114</span></a><span class="sd">        image under a m-transform. If unspecified TPCA will use the Discrete</span>
</span><span id="tsvdm-115"><a href="#tsvdm-115"><span class="linenos">115</span></a><span class="sd">        Cosine Transform (ii) from `scipy.fft`.</span>
</span><span id="tsvdm-116"><a href="#tsvdm-116"><span class="linenos">116</span></a>
</span><span id="tsvdm-117"><a href="#tsvdm-117"><span class="linenos">117</span></a><span class="sd">    MInv : Callable[[ndarray], ndarray] or None, default=None</span>
</span><span id="tsvdm-118"><a href="#tsvdm-118"><span class="linenos">118</span></a><span class="sd">        A function implementing the inverse transform of `M`.</span>
</span><span id="tsvdm-119"><a href="#tsvdm-119"><span class="linenos">119</span></a>
</span><span id="tsvdm-120"><a href="#tsvdm-120"><span class="linenos">120</span></a><span class="sd">    keep_hats : bool, default=False</span>
</span><span id="tsvdm-121"><a href="#tsvdm-121"><span class="linenos">121</span></a><span class="sd">        Setting to `True` will return the tSVDM factors in the m-transform</span>
</span><span id="tsvdm-122"><a href="#tsvdm-122"><span class="linenos">122</span></a><span class="sd">        space, under the specified `M`</span>
</span><span id="tsvdm-123"><a href="#tsvdm-123"><span class="linenos">123</span></a>
</span><span id="tsvdm-124"><a href="#tsvdm-124"><span class="linenos">124</span></a><span class="sd">    full_frontal_slices : bool, default=True</span>
</span><span id="tsvdm-125"><a href="#tsvdm-125"><span class="linenos">125</span></a><span class="sd">        To reconstruct `A`, one only needs the first $k$ columns of</span>
</span><span id="tsvdm-126"><a href="#tsvdm-126"><span class="linenos">126</span></a><span class="sd">        $U$ and $V$.</span>
</span><span id="tsvdm-127"><a href="#tsvdm-127"><span class="linenos">127</span></a>
</span><span id="tsvdm-128"><a href="#tsvdm-128"><span class="linenos">128</span></a><span class="sd">        Setting this to False will return the truncated tensors.</span>
</span><span id="tsvdm-129"><a href="#tsvdm-129"><span class="linenos">129</span></a><span class="sd">        See: https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html</span>
</span><span id="tsvdm-130"><a href="#tsvdm-130"><span class="linenos">130</span></a>
</span><span id="tsvdm-131"><a href="#tsvdm-131"><span class="linenos">131</span></a><span class="sd">    svals_matrix_form : bool, default=False</span>
</span><span id="tsvdm-132"><a href="#tsvdm-132"><span class="linenos">132</span></a><span class="sd">        Setting to `True` will return a compressed version of $S$, where the</span>
</span><span id="tsvdm-133"><a href="#tsvdm-133"><span class="linenos">133</span></a><span class="sd">        singular values of each f-diagonal frontal slice becomes the column of</span>
</span><span id="tsvdm-134"><a href="#tsvdm-134"><span class="linenos">134</span></a><span class="sd">        a matrix, with `t` columns total.</span>
</span><span id="tsvdm-135"><a href="#tsvdm-135"><span class="linenos">135</span></a>
</span><span id="tsvdm-136"><a href="#tsvdm-136"><span class="linenos">136</span></a><span class="sd">    Returns</span>
</span><span id="tsvdm-137"><a href="#tsvdm-137"><span class="linenos">137</span></a><span class="sd">    -------</span>
</span><span id="tsvdm-138"><a href="#tsvdm-138"><span class="linenos">138</span></a><span class="sd">    U_tens : ndarray, shape: (n, n, t)</span>
</span><span id="tsvdm-139"><a href="#tsvdm-139"><span class="linenos">139</span></a><span class="sd">        If `full_frontal_slices==False` shape is (n, k, t) instead</span>
</span><span id="tsvdm-140"><a href="#tsvdm-140"><span class="linenos">140</span></a>
</span><span id="tsvdm-141"><a href="#tsvdm-141"><span class="linenos">141</span></a><span class="sd">    S_tens : ndarray, shape: (n, p, t)</span>
</span><span id="tsvdm-142"><a href="#tsvdm-142"><span class="linenos">142</span></a><span class="sd">        If `full_frontal_slices==False` shape is (k, k, t) instead</span>
</span><span id="tsvdm-143"><a href="#tsvdm-143"><span class="linenos">143</span></a><span class="sd">        If `svals_matrix_form==True`, `S_mat` of shape (k, t) returned instead</span>
</span><span id="tsvdm-144"><a href="#tsvdm-144"><span class="linenos">144</span></a>
</span><span id="tsvdm-145"><a href="#tsvdm-145"><span class="linenos">145</span></a><span class="sd">    V_tens : ndarray, shape: (p, p, t)</span>
</span><span id="tsvdm-146"><a href="#tsvdm-146"><span class="linenos">146</span></a><span class="sd">        If `full_frontal_slices==False` shape is (p, k, t) instead</span>
</span><span id="tsvdm-147"><a href="#tsvdm-147"><span class="linenos">147</span></a>
</span><span id="tsvdm-148"><a href="#tsvdm-148"><span class="linenos">148</span></a><span class="sd">    References</span>
</span><span id="tsvdm-149"><a href="#tsvdm-149"><span class="linenos">149</span></a><span class="sd">    ----------</span>
</span><span id="tsvdm-150"><a href="#tsvdm-150"><span class="linenos">150</span></a><span class="sd">    Kilmer, M.E., Horesh, L., Avron, H. and Newman, E., 2021. Tensor-tensor</span>
</span><span id="tsvdm-151"><a href="#tsvdm-151"><span class="linenos">151</span></a><span class="sd">    algebra for optimal representation and compression of multiway data.</span>
</span><span id="tsvdm-152"><a href="#tsvdm-152"><span class="linenos">152</span></a><span class="sd">    Proceedings of the National Academy of Sciences, 118(28), p.e2015851118.</span>
</span><span id="tsvdm-153"><a href="#tsvdm-153"><span class="linenos">153</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="tsvdm-154"><a href="#tsvdm-154"><span class="linenos">154</span></a>
</span><span id="tsvdm-155"><a href="#tsvdm-155"><span class="linenos">155</span></a>    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;Ensure order-3 tensor input&quot;</span>
</span><span id="tsvdm-156"><a href="#tsvdm-156"><span class="linenos">156</span></a>    <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span>
</span><span id="tsvdm-157"><a href="#tsvdm-157"><span class="linenos">157</span></a>        <span class="nb">callable</span><span class="p">(</span><span class="n">M</span><span class="p">)</span> <span class="o">^</span> <span class="nb">callable</span><span class="p">(</span><span class="n">Minv</span><span class="p">)</span>
</span><span id="tsvdm-158"><a href="#tsvdm-158"><span class="linenos">158</span></a>    <span class="p">),</span> <span class="s2">&quot;If explicitly defined, both M and its inverse must be defined&quot;</span>
</span><span id="tsvdm-159"><a href="#tsvdm-159"><span class="linenos">159</span></a>
</span><span id="tsvdm-160"><a href="#tsvdm-160"><span class="linenos">160</span></a>    <span class="k">if</span> <span class="ow">not</span> <span class="nb">callable</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>  <span class="c1"># and Minv is not defined - guaranteed by assertion</span>
</span><span id="tsvdm-161"><a href="#tsvdm-161"><span class="linenos">161</span></a>        <span class="n">M</span><span class="p">,</span> <span class="n">Minv</span> <span class="o">=</span> <span class="n">generate_default_m_transform_pair</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span><span id="tsvdm-162"><a href="#tsvdm-162"><span class="linenos">162</span></a>
</span><span id="tsvdm-163"><a href="#tsvdm-163"><span class="linenos">163</span></a>    <span class="c1"># transform the tensor to new space via the mode-3 product</span>
</span><span id="tsvdm-164"><a href="#tsvdm-164"><span class="linenos">164</span></a>    <span class="n">hatA</span> <span class="o">=</span> <span class="n">M</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</span><span id="tsvdm-165"><a href="#tsvdm-165"><span class="linenos">165</span></a>
</span><span id="tsvdm-166"><a href="#tsvdm-166"><span class="linenos">166</span></a>    <span class="c1"># an appropriate transposition allows Numpys array broadcasting to do</span>
</span><span id="tsvdm-167"><a href="#tsvdm-167"><span class="linenos">167</span></a>    <span class="c1"># facewise svd&#39;s S_mat contains the singular values per matrix in the</span>
</span><span id="tsvdm-168"><a href="#tsvdm-168"><span class="linenos">168</span></a>    <span class="c1"># input stack of matrices (the transpose tensor stacks top to bottom,</span>
</span><span id="tsvdm-169"><a href="#tsvdm-169"><span class="linenos">169</span></a>    <span class="c1"># with t slices of size n by p)</span>
</span><span id="tsvdm-170"><a href="#tsvdm-170"><span class="linenos">170</span></a>    <span class="n">U_stack</span><span class="p">,</span> <span class="n">S_mat</span><span class="p">,</span> <span class="n">Vt_stack</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span>
</span><span id="tsvdm-171"><a href="#tsvdm-171"><span class="linenos">171</span></a>        <span class="n">hatA</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">full_matrices</span><span class="o">=</span><span class="n">full_frontal_slices</span>
</span><span id="tsvdm-172"><a href="#tsvdm-172"><span class="linenos">172</span></a>    <span class="p">)</span>
</span><span id="tsvdm-173"><a href="#tsvdm-173"><span class="linenos">173</span></a>
</span><span id="tsvdm-174"><a href="#tsvdm-174"><span class="linenos">174</span></a>    <span class="n">hatU</span> <span class="o">=</span> <span class="n">U_stack</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="tsvdm-175"><a href="#tsvdm-175"><span class="linenos">175</span></a>    <span class="n">S_mat</span> <span class="o">=</span> <span class="n">S_mat</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
</span><span id="tsvdm-176"><a href="#tsvdm-176"><span class="linenos">176</span></a>    <span class="c1"># the following is a call to .transpose(1, 2, 0) followed by a facewise</span>
</span><span id="tsvdm-177"><a href="#tsvdm-177"><span class="linenos">177</span></a>    <span class="c1"># transpose defined by .transpose(1, 0, 2)</span>
</span><span id="tsvdm-178"><a href="#tsvdm-178"><span class="linenos">178</span></a>    <span class="n">hatV</span> <span class="o">=</span> <span class="n">Vt_stack</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="tsvdm-179"><a href="#tsvdm-179"><span class="linenos">179</span></a>
</span><span id="tsvdm-180"><a href="#tsvdm-180"><span class="linenos">180</span></a>    <span class="c1"># if we are transforming scipy&#39;s singular values matrix back into tensor</span>
</span><span id="tsvdm-181"><a href="#tsvdm-181"><span class="linenos">181</span></a>    <span class="c1"># form, make sure we use the correct dimensions corresponding to whether</span>
</span><span id="tsvdm-182"><a href="#tsvdm-182"><span class="linenos">182</span></a>    <span class="c1"># or not the tensor faces were truncated during svd</span>
</span><span id="tsvdm-183"><a href="#tsvdm-183"><span class="linenos">183</span></a>    <span class="k">if</span> <span class="ow">not</span> <span class="n">svals_matrix_form</span><span class="p">:</span>
</span><span id="tsvdm-184"><a href="#tsvdm-184"><span class="linenos">184</span></a>        <span class="k">if</span> <span class="n">full_frontal_slices</span><span class="p">:</span>
</span><span id="tsvdm-185"><a href="#tsvdm-185"><span class="linenos">185</span></a>            <span class="n">desired_S_tens_shape</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span>
</span><span id="tsvdm-186"><a href="#tsvdm-186"><span class="linenos">186</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="tsvdm-187"><a href="#tsvdm-187"><span class="linenos">187</span></a>            <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span>
</span><span id="tsvdm-188"><a href="#tsvdm-188"><span class="linenos">188</span></a>            <span class="n">k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</span><span id="tsvdm-189"><a href="#tsvdm-189"><span class="linenos">189</span></a>            <span class="n">desired_S_tens_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span><span id="tsvdm-190"><a href="#tsvdm-190"><span class="linenos">190</span></a>
</span><span id="tsvdm-191"><a href="#tsvdm-191"><span class="linenos">191</span></a>    <span class="k">if</span> <span class="n">keep_hats</span><span class="p">:</span>
</span><span id="tsvdm-192"><a href="#tsvdm-192"><span class="linenos">192</span></a>        <span class="k">return</span> <span class="p">(</span>
</span><span id="tsvdm-193"><a href="#tsvdm-193"><span class="linenos">193</span></a>            <span class="n">hatU</span><span class="p">,</span>
</span><span id="tsvdm-194"><a href="#tsvdm-194"><span class="linenos">194</span></a>            <span class="c1"># by default return S as n,p,t f-diagonal tensor (or) convert into</span>
</span><span id="tsvdm-195"><a href="#tsvdm-195"><span class="linenos">195</span></a>            <span class="c1"># compressed matrix of singular values of shape (k,t)</span>
</span><span id="tsvdm-196"><a href="#tsvdm-196"><span class="linenos">196</span></a>            <span class="n">S_mat</span>
</span><span id="tsvdm-197"><a href="#tsvdm-197"><span class="linenos">197</span></a>            <span class="k">if</span> <span class="n">svals_matrix_form</span>
</span><span id="tsvdm-198"><a href="#tsvdm-198"><span class="linenos">198</span></a>            <span class="k">else</span> <span class="n">_singular_vals_mat_to_tensor</span><span class="p">(</span><span class="n">S_mat</span><span class="p">,</span> <span class="o">*</span><span class="n">desired_S_tens_shape</span><span class="p">),</span>
</span><span id="tsvdm-199"><a href="#tsvdm-199"><span class="linenos">199</span></a>            <span class="n">hatV</span><span class="p">,</span>
</span><span id="tsvdm-200"><a href="#tsvdm-200"><span class="linenos">200</span></a>        <span class="p">)</span>
</span><span id="tsvdm-201"><a href="#tsvdm-201"><span class="linenos">201</span></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="tsvdm-202"><a href="#tsvdm-202"><span class="linenos">202</span></a>        <span class="k">return</span> <span class="p">(</span>
</span><span id="tsvdm-203"><a href="#tsvdm-203"><span class="linenos">203</span></a>            <span class="n">Minv</span><span class="p">(</span><span class="n">hatU</span><span class="p">),</span>
</span><span id="tsvdm-204"><a href="#tsvdm-204"><span class="linenos">204</span></a>            <span class="c1"># by default return S as n,p,t f-diagonal tensor (or) convert into</span>
</span><span id="tsvdm-205"><a href="#tsvdm-205"><span class="linenos">205</span></a>            <span class="c1"># compressed matrix of singular values of shape (k,t)</span>
</span><span id="tsvdm-206"><a href="#tsvdm-206"><span class="linenos">206</span></a>            <span class="n">Minv</span><span class="p">(</span><span class="n">S_mat</span><span class="p">)</span>
</span><span id="tsvdm-207"><a href="#tsvdm-207"><span class="linenos">207</span></a>            <span class="k">if</span> <span class="n">svals_matrix_form</span>
</span><span id="tsvdm-208"><a href="#tsvdm-208"><span class="linenos">208</span></a>            <span class="k">else</span> <span class="n">_singular_vals_mat_to_tensor</span><span class="p">(</span><span class="n">Minv</span><span class="p">(</span><span class="n">S_mat</span><span class="p">),</span> <span class="o">*</span><span class="n">desired_S_tens_shape</span><span class="p">),</span>
</span><span id="tsvdm-209"><a href="#tsvdm-209"><span class="linenos">209</span></a>            <span class="n">Minv</span><span class="p">(</span><span class="n">hatV</span><span class="p">),</span>
</span><span id="tsvdm-210"><a href="#tsvdm-210"><span class="linenos">210</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Return the t-SVDM decomposition from Kilmer et al. (2021).</p>

<p>This is a modified version, inspired by the implementation at
<a href="https://github.com/UriaMorP/mprod_package">https://github.com/UriaMorP/mprod_package</a></p>

<p><em>NOTE: For now, unlike some other implementations (Numpy, Scipy), we
will return the tensor $V$ NOT $V^T$.</em></p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>A : ndarray, shape</strong> ((n, p, t)):
Data tensor</li>
<li><strong>M</strong> (Callable[[ndarray], ndarray] or None, default=None):
A function which expects an order-3 tensor as input, and returns the
image under a m-transform. If unspecified TPCA will use the Discrete
Cosine Transform (ii) from <code>scipy.fft</code>.</li>
<li><strong>MInv</strong> (Callable[[ndarray], ndarray] or None, default=None):
A function implementing the inverse transform of <code>M</code>.</li>
<li><strong>keep_hats</strong> (bool, default=False):
Setting to <code>True</code> will return the tSVDM factors in the m-transform
space, under the specified <code>M</code></li>
<li><p><strong>full_frontal_slices</strong> (bool, default=True):
To reconstruct <code>A</code>, one only needs the first $k$ columns of
$U$ and $V$.</p>

<p>Setting this to False will return the truncated tensors.
See: <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html">https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html</a></p></li>
<li><strong>svals_matrix_form</strong> (bool, default=False):
Setting to <code>True</code> will return a compressed version of $S$, where the
singular values of each f-diagonal frontal slice becomes the column of
a matrix, with <code>t</code> columns total.</li>
</ul>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>U_tens : ndarray, shape</strong> ((n, n, t)):
If <code>full_frontal_slices==False</code> shape is (n, k, t) instead</li>
<li><strong>S_tens : ndarray, shape</strong> ((n, p, t)):
If <code>full_frontal_slices==False</code> shape is (k, k, t) instead
If <code>svals_matrix_form==True</code>, <code>S_mat</code> of shape (k, t) returned instead</li>
<li><strong>V_tens : ndarray, shape</strong> ((p, p, t)):
If <code>full_frontal_slices==False</code> shape is (p, k, t) instead</li>
</ul>

<h6 id="references">References</h6>

<p>Kilmer, M.E., Horesh, L., Avron, H. and Newman, E., 2021. Tensor-tensor
algebra for optimal representation and compression of multiway data.
Proceedings of the National Academy of Sciences, 118(28), p.e2015851118.</p>
</div>


                </section>
                <section id="TPCA">
                            <input id="TPCA-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">TPCA</span><wbr>(<span class="base">sklearn.base.ClassNamePrefixFeaturesOutMixin</span>, <span class="base">sklearn.base.TransformerMixin</span>, <span class="base">sklearn.base.BaseEstimator</span>):

                <label class="view-source-button" for="TPCA-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#TPCA"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="TPCA-25"><a href="#TPCA-25"><span class="linenos"> 25</span></a><span class="k">class</span> <span class="nc">TPCA</span><span class="p">(</span><span class="n">ClassNamePrefixFeaturesOutMixin</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">):</span>
</span><span id="TPCA-26"><a href="#TPCA-26"><span class="linenos"> 26</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Tensor analogue of PCA, introduced by Mor et al. (2022).</span>
</span><span id="TPCA-27"><a href="#TPCA-27"><span class="linenos"> 27</span></a>
</span><span id="TPCA-28"><a href="#TPCA-28"><span class="linenos"> 28</span></a><span class="sd">    t-SVDM tensor analogue of PCA using explicit rank truncation with explicit</span>
</span><span id="TPCA-29"><a href="#TPCA-29"><span class="linenos"> 29</span></a><span class="sd">    rank truncation from Mor et al. (2022), and underlying m-product framework</span>
</span><span id="TPCA-30"><a href="#TPCA-30"><span class="linenos"> 30</span></a><span class="sd">    from Kilmer et al. (2021). Takes in an $n \\times p \\times t$ input</span>
</span><span id="TPCA-31"><a href="#TPCA-31"><span class="linenos"> 31</span></a><span class="sd">    tensor, and transforms into a $n \\times$ `n_components` matrix of 2D</span>
</span><span id="TPCA-32"><a href="#TPCA-32"><span class="linenos"> 32</span></a><span class="sd">    transformed projections.</span>
</span><span id="TPCA-33"><a href="#TPCA-33"><span class="linenos"> 33</span></a>
</span><span id="TPCA-34"><a href="#TPCA-34"><span class="linenos"> 34</span></a><span class="sd">    The input tensor is centred into Mean Deviation Form (by location), but</span>
</span><span id="TPCA-35"><a href="#TPCA-35"><span class="linenos"> 35</span></a><span class="sd">    not normalized (by scale).</span>
</span><span id="TPCA-36"><a href="#TPCA-36"><span class="linenos"> 36</span></a>
</span><span id="TPCA-37"><a href="#TPCA-37"><span class="linenos"> 37</span></a><span class="sd">    Parameters</span>
</span><span id="TPCA-38"><a href="#TPCA-38"><span class="linenos"> 38</span></a><span class="sd">    ----------</span>
</span><span id="TPCA-39"><a href="#TPCA-39"><span class="linenos"> 39</span></a><span class="sd">    n_components : int, float, or None, default=None</span>
</span><span id="TPCA-40"><a href="#TPCA-40"><span class="linenos"> 40</span></a><span class="sd">        Control number of components to keep. If n_components is not set at</span>
</span><span id="TPCA-41"><a href="#TPCA-41"><span class="linenos"> 41</span></a><span class="sd">        all, or set to `None`, ``n_components == min(n, p) * t``</span>
</span><span id="TPCA-42"><a href="#TPCA-42"><span class="linenos"> 42</span></a>
</span><span id="TPCA-43"><a href="#TPCA-43"><span class="linenos"> 43</span></a><span class="sd">        If `n_components` is an integer, the TPCA will return the number of</span>
</span><span id="TPCA-44"><a href="#TPCA-44"><span class="linenos"> 44</span></a><span class="sd">        loadings, provided that for the tensor data passed into `fit`,</span>
</span><span id="TPCA-45"><a href="#TPCA-45"><span class="linenos"> 45</span></a><span class="sd">        satisfies ``1 &lt;= n_components &lt;= min(n, p) * t``</span>
</span><span id="TPCA-46"><a href="#TPCA-46"><span class="linenos"> 46</span></a>
</span><span id="TPCA-47"><a href="#TPCA-47"><span class="linenos"> 47</span></a><span class="sd">        If ``0 &lt; n_components &lt; 1``, TPCA will select the number of</span>
</span><span id="TPCA-48"><a href="#TPCA-48"><span class="linenos"> 48</span></a><span class="sd">        compononents such that the amount of variance that needs to be</span>
</span><span id="TPCA-49"><a href="#TPCA-49"><span class="linenos"> 49</span></a><span class="sd">        explained is greater than the percentage specified.</span>
</span><span id="TPCA-50"><a href="#TPCA-50"><span class="linenos"> 50</span></a>
</span><span id="TPCA-51"><a href="#TPCA-51"><span class="linenos"> 51</span></a><span class="sd">    copy : bool, default=True</span>
</span><span id="TPCA-52"><a href="#TPCA-52"><span class="linenos"> 52</span></a><span class="sd">        If False, data passed to fit are overwritten and running</span>
</span><span id="TPCA-53"><a href="#TPCA-53"><span class="linenos"> 53</span></a><span class="sd">        `fit(X).transform(X)` will not yield the expected results. Use</span>
</span><span id="TPCA-54"><a href="#TPCA-54"><span class="linenos"> 54</span></a><span class="sd">        `fit_transform(X)` instead.</span>
</span><span id="TPCA-55"><a href="#TPCA-55"><span class="linenos"> 55</span></a>
</span><span id="TPCA-56"><a href="#TPCA-56"><span class="linenos"> 56</span></a><span class="sd">    M : Callable[[ndarray], ndarray] or None, default=None</span>
</span><span id="TPCA-57"><a href="#TPCA-57"><span class="linenos"> 57</span></a><span class="sd">        A function which expects an order-3 tensor as input, and returns the</span>
</span><span id="TPCA-58"><a href="#TPCA-58"><span class="linenos"> 58</span></a><span class="sd">        image under a m-transform. If unspecified TPCA will use the Discrete</span>
</span><span id="TPCA-59"><a href="#TPCA-59"><span class="linenos"> 59</span></a><span class="sd">        Cosine Transform (ii) from `scipy.fft`.</span>
</span><span id="TPCA-60"><a href="#TPCA-60"><span class="linenos"> 60</span></a>
</span><span id="TPCA-61"><a href="#TPCA-61"><span class="linenos"> 61</span></a><span class="sd">    Minv : Callable[[ndarray], ndarray] or None, default=None</span>
</span><span id="TPCA-62"><a href="#TPCA-62"><span class="linenos"> 62</span></a><span class="sd">        A function implementing the inverse transform of `M`.</span>
</span><span id="TPCA-63"><a href="#TPCA-63"><span class="linenos"> 63</span></a>
</span><span id="TPCA-64"><a href="#TPCA-64"><span class="linenos"> 64</span></a><span class="sd">    centre : bool, default=True</span>
</span><span id="TPCA-65"><a href="#TPCA-65"><span class="linenos"> 65</span></a><span class="sd">        If False, the data tensor will not be centralized into Mean Deviation</span>
</span><span id="TPCA-66"><a href="#TPCA-66"><span class="linenos"> 66</span></a><span class="sd">        Form. By default, the mean horizontal slice of the tensor is</span>
</span><span id="TPCA-67"><a href="#TPCA-67"><span class="linenos"> 67</span></a><span class="sd">        subtracted, so that all of the horizontal slices sum to 0, analagous</span>
</span><span id="TPCA-68"><a href="#TPCA-68"><span class="linenos"> 68</span></a><span class="sd">        to centering the data in PCA.</span>
</span><span id="TPCA-69"><a href="#TPCA-69"><span class="linenos"> 69</span></a>
</span><span id="TPCA-70"><a href="#TPCA-70"><span class="linenos"> 70</span></a><span class="sd">    Attributes</span>
</span><span id="TPCA-71"><a href="#TPCA-71"><span class="linenos"> 71</span></a><span class="sd">    ----------</span>
</span><span id="TPCA-72"><a href="#TPCA-72"><span class="linenos"> 72</span></a><span class="sd">    n_, p_, t_, k_ : int</span>
</span><span id="TPCA-73"><a href="#TPCA-73"><span class="linenos"> 73</span></a><span class="sd">        The dimensions of the training data. ``k_ == min(n_, p_)``</span>
</span><span id="TPCA-74"><a href="#TPCA-74"><span class="linenos"> 74</span></a>
</span><span id="TPCA-75"><a href="#TPCA-75"><span class="linenos"> 75</span></a><span class="sd">    M_, MInv_ : Callable[[ndarray], ndarray]</span>
</span><span id="TPCA-76"><a href="#TPCA-76"><span class="linenos"> 76</span></a><span class="sd">        The m-transform pair (forward and inverse) used for the underlying</span>
</span><span id="TPCA-77"><a href="#TPCA-77"><span class="linenos"> 77</span></a><span class="sd">        tensor-tensor m-product.</span>
</span><span id="TPCA-78"><a href="#TPCA-78"><span class="linenos"> 78</span></a>
</span><span id="TPCA-79"><a href="#TPCA-79"><span class="linenos"> 79</span></a><span class="sd">    n_components_ : int</span>
</span><span id="TPCA-80"><a href="#TPCA-80"><span class="linenos"> 80</span></a><span class="sd">        The estimated number of components. If `n_components` was explicitly</span>
</span><span id="TPCA-81"><a href="#TPCA-81"><span class="linenos"> 81</span></a><span class="sd">        set by an integer value, this will be the same as that. If</span>
</span><span id="TPCA-82"><a href="#TPCA-82"><span class="linenos"> 82</span></a><span class="sd">        `n_components` was a number between 0 and 1, this number is estimated</span>
</span><span id="TPCA-83"><a href="#TPCA-83"><span class="linenos"> 83</span></a><span class="sd">        from input data. Otherwise, if not set (defaults to None), it will</span>
</span><span id="TPCA-84"><a href="#TPCA-84"><span class="linenos"> 84</span></a><span class="sd">        default to $k \\times t$ in the training data.</span>
</span><span id="TPCA-85"><a href="#TPCA-85"><span class="linenos"> 85</span></a>
</span><span id="TPCA-86"><a href="#TPCA-86"><span class="linenos"> 86</span></a><span class="sd">    explained_variance_ratio_ : ndarray of shape (n_components_,)</span>
</span><span id="TPCA-87"><a href="#TPCA-87"><span class="linenos"> 87</span></a><span class="sd">        Percentage of total variance explained by each of the selected</span>
</span><span id="TPCA-88"><a href="#TPCA-88"><span class="linenos"> 88</span></a><span class="sd">        components. The selected components are selected so that this is</span>
</span><span id="TPCA-89"><a href="#TPCA-89"><span class="linenos"> 89</span></a><span class="sd">        returned in descending order.</span>
</span><span id="TPCA-90"><a href="#TPCA-90"><span class="linenos"> 90</span></a>
</span><span id="TPCA-91"><a href="#TPCA-91"><span class="linenos"> 91</span></a><span class="sd">        If `n_components` is not set then all components are stored and</span>
</span><span id="TPCA-92"><a href="#TPCA-92"><span class="linenos"> 92</span></a><span class="sd">        the sum of this ratios array is equal to 1.0.</span>
</span><span id="TPCA-93"><a href="#TPCA-93"><span class="linenos"> 93</span></a>
</span><span id="TPCA-94"><a href="#TPCA-94"><span class="linenos"> 94</span></a><span class="sd">    singular_values_ : ndarray of shape (n_components,)</span>
</span><span id="TPCA-95"><a href="#TPCA-95"><span class="linenos"> 95</span></a><span class="sd">        The singular values corresponding to each of the selected components.</span>
</span><span id="TPCA-96"><a href="#TPCA-96"><span class="linenos"> 96</span></a>
</span><span id="TPCA-97"><a href="#TPCA-97"><span class="linenos"> 97</span></a><span class="sd">    mean_ : ndarray of shape (p_, t_)</span>
</span><span id="TPCA-98"><a href="#TPCA-98"><span class="linenos"> 98</span></a><span class="sd">        Per-feature, per-timepoint empirical mean, estimated from the</span>
</span><span id="TPCA-99"><a href="#TPCA-99"><span class="linenos"> 99</span></a><span class="sd">        training set. This is used to normalize any new data passed to</span>
</span><span id="TPCA-100"><a href="#TPCA-100"><span class="linenos">100</span></a><span class="sd">        `transform(X)`, unless centre is explicitly turned off via</span>
</span><span id="TPCA-101"><a href="#TPCA-101"><span class="linenos">101</span></a><span class="sd">        ``centre==False`` during object instantiation.</span>
</span><span id="TPCA-102"><a href="#TPCA-102"><span class="linenos">102</span></a>
</span><span id="TPCA-103"><a href="#TPCA-103"><span class="linenos">103</span></a><span class="sd">    rho_ : ndarray of shape (t,)</span>
</span><span id="TPCA-104"><a href="#TPCA-104"><span class="linenos">104</span></a><span class="sd">        The rho used in multi-rank truncation to achieve the desired explicit</span>
</span><span id="TPCA-105"><a href="#TPCA-105"><span class="linenos">105</span></a><span class="sd">        rank of ``n_components``. See Mor et al. (2022) for detail.</span>
</span><span id="TPCA-106"><a href="#TPCA-106"><span class="linenos">106</span></a>
</span><span id="TPCA-107"><a href="#TPCA-107"><span class="linenos">107</span></a><span class="sd">    loadings_matrix_ : ndarray of shape (n_components_, p_)</span>
</span><span id="TPCA-108"><a href="#TPCA-108"><span class="linenos">108</span></a><span class="sd">        The i-th row corresponds to the column of $\\hat{V}$ which contains</span>
</span><span id="TPCA-109"><a href="#TPCA-109"><span class="linenos">109</span></a><span class="sd">        the feature weights applied to the data (in the hat-space) to get the</span>
</span><span id="TPCA-110"><a href="#TPCA-110"><span class="linenos">110</span></a><span class="sd">        i-th TPCA component.</span>
</span><span id="TPCA-111"><a href="#TPCA-111"><span class="linenos">111</span></a>
</span><span id="TPCA-112"><a href="#TPCA-112"><span class="linenos">112</span></a><span class="sd">    References</span>
</span><span id="TPCA-113"><a href="#TPCA-113"><span class="linenos">113</span></a><span class="sd">    ----------</span>
</span><span id="TPCA-114"><a href="#TPCA-114"><span class="linenos">114</span></a><span class="sd">    Mor, U., Cohen, Y., Valdés-Mas, R., Kviatcovsky, D., Elinav, E. and Avron,</span>
</span><span id="TPCA-115"><a href="#TPCA-115"><span class="linenos">115</span></a><span class="sd">    H., 2022. Dimensionality reduction of longitudinal’omics data using modern</span>
</span><span id="TPCA-116"><a href="#TPCA-116"><span class="linenos">116</span></a><span class="sd">    tensor factorizations. PLoS Computational Biology, 18(7), p.e1010212.</span>
</span><span id="TPCA-117"><a href="#TPCA-117"><span class="linenos">117</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="TPCA-118"><a href="#TPCA-118"><span class="linenos">118</span></a>
</span><span id="TPCA-119"><a href="#TPCA-119"><span class="linenos">119</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">Minv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">centre</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span><span id="TPCA-120"><a href="#TPCA-120"><span class="linenos">120</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;@private</span>
</span><span id="TPCA-121"><a href="#TPCA-121"><span class="linenos">121</span></a><span class="sd">        Hacky way (for now) to suppress pdoc documentation being generated</span>
</span><span id="TPCA-122"><a href="#TPCA-122"><span class="linenos">122</span></a><span class="sd">        for the instance variables and the constructor&quot;&quot;&quot;</span>
</span><span id="TPCA-123"><a href="#TPCA-123"><span class="linenos">123</span></a>        <span class="c1"># as per sklearn conventions, we perform any and all parameter</span>
</span><span id="TPCA-124"><a href="#TPCA-124"><span class="linenos">124</span></a>        <span class="c1"># validation inside fit, and none in __init__</span>
</span><span id="TPCA-125"><a href="#TPCA-125"><span class="linenos">125</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="o">=</span> <span class="n">n_components</span>
</span><span id="TPCA-126"><a href="#TPCA-126"><span class="linenos">126</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;@private&quot;&quot;&quot;</span>
</span><span id="TPCA-127"><a href="#TPCA-127"><span class="linenos">127</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">copy</span> <span class="o">=</span> <span class="n">copy</span>
</span><span id="TPCA-128"><a href="#TPCA-128"><span class="linenos">128</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;@private&quot;&quot;&quot;</span>
</span><span id="TPCA-129"><a href="#TPCA-129"><span class="linenos">129</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">M</span> <span class="o">=</span> <span class="n">M</span>
</span><span id="TPCA-130"><a href="#TPCA-130"><span class="linenos">130</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;@private&quot;&quot;&quot;</span>
</span><span id="TPCA-131"><a href="#TPCA-131"><span class="linenos">131</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">Minv</span> <span class="o">=</span> <span class="n">Minv</span>
</span><span id="TPCA-132"><a href="#TPCA-132"><span class="linenos">132</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;@private&quot;&quot;&quot;</span>
</span><span id="TPCA-133"><a href="#TPCA-133"><span class="linenos">133</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">centre</span> <span class="o">=</span> <span class="n">centre</span>
</span><span id="TPCA-134"><a href="#TPCA-134"><span class="linenos">134</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;@private&quot;&quot;&quot;</span>
</span><span id="TPCA-135"><a href="#TPCA-135"><span class="linenos">135</span></a>
</span><span id="TPCA-136"><a href="#TPCA-136"><span class="linenos">136</span></a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span><span id="TPCA-137"><a href="#TPCA-137"><span class="linenos">137</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Fit the model with X.</span>
</span><span id="TPCA-138"><a href="#TPCA-138"><span class="linenos">138</span></a>
</span><span id="TPCA-139"><a href="#TPCA-139"><span class="linenos">139</span></a><span class="sd">        Parameters</span>
</span><span id="TPCA-140"><a href="#TPCA-140"><span class="linenos">140</span></a><span class="sd">        ----------</span>
</span><span id="TPCA-141"><a href="#TPCA-141"><span class="linenos">141</span></a><span class="sd">        X : ndarray of shape (n, p, t)</span>
</span><span id="TPCA-142"><a href="#TPCA-142"><span class="linenos">142</span></a><span class="sd">            Training data, where `n` is the number of samples, `p` is the</span>
</span><span id="TPCA-143"><a href="#TPCA-143"><span class="linenos">143</span></a><span class="sd">            number of features, as `t` is the number of time points.</span>
</span><span id="TPCA-144"><a href="#TPCA-144"><span class="linenos">144</span></a>
</span><span id="TPCA-145"><a href="#TPCA-145"><span class="linenos">145</span></a><span class="sd">        y : Ignored</span>
</span><span id="TPCA-146"><a href="#TPCA-146"><span class="linenos">146</span></a><span class="sd">            Ignored.</span>
</span><span id="TPCA-147"><a href="#TPCA-147"><span class="linenos">147</span></a>
</span><span id="TPCA-148"><a href="#TPCA-148"><span class="linenos">148</span></a><span class="sd">        Returns</span>
</span><span id="TPCA-149"><a href="#TPCA-149"><span class="linenos">149</span></a><span class="sd">        -------</span>
</span><span id="TPCA-150"><a href="#TPCA-150"><span class="linenos">150</span></a><span class="sd">        self : object</span>
</span><span id="TPCA-151"><a href="#TPCA-151"><span class="linenos">151</span></a><span class="sd">            Returns the instance itself, after being fitted.</span>
</span><span id="TPCA-152"><a href="#TPCA-152"><span class="linenos">152</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="TPCA-153"><a href="#TPCA-153"><span class="linenos">153</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="TPCA-154"><a href="#TPCA-154"><span class="linenos">154</span></a>        <span class="k">return</span> <span class="bp">self</span>
</span><span id="TPCA-155"><a href="#TPCA-155"><span class="linenos">155</span></a>
</span><span id="TPCA-156"><a href="#TPCA-156"><span class="linenos">156</span></a>    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="TPCA-157"><a href="#TPCA-157"><span class="linenos">157</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply dimensionality reduction to X.</span>
</span><span id="TPCA-158"><a href="#TPCA-158"><span class="linenos">158</span></a>
</span><span id="TPCA-159"><a href="#TPCA-159"><span class="linenos">159</span></a><span class="sd">        See the TCAM algorithm from Mor et al. (2022)</span>
</span><span id="TPCA-160"><a href="#TPCA-160"><span class="linenos">160</span></a>
</span><span id="TPCA-161"><a href="#TPCA-161"><span class="linenos">161</span></a><span class="sd">        Parameters</span>
</span><span id="TPCA-162"><a href="#TPCA-162"><span class="linenos">162</span></a><span class="sd">        ----------</span>
</span><span id="TPCA-163"><a href="#TPCA-163"><span class="linenos">163</span></a><span class="sd">        X : ndarray of shape (n, p, t)</span>
</span><span id="TPCA-164"><a href="#TPCA-164"><span class="linenos">164</span></a><span class="sd">            Training data, where `n` is the number of samples, `p` is the</span>
</span><span id="TPCA-165"><a href="#TPCA-165"><span class="linenos">165</span></a><span class="sd">            number of features, as `t` is the number of time points.</span>
</span><span id="TPCA-166"><a href="#TPCA-166"><span class="linenos">166</span></a>
</span><span id="TPCA-167"><a href="#TPCA-167"><span class="linenos">167</span></a><span class="sd">        y : Ignored</span>
</span><span id="TPCA-168"><a href="#TPCA-168"><span class="linenos">168</span></a><span class="sd">            Ignored.</span>
</span><span id="TPCA-169"><a href="#TPCA-169"><span class="linenos">169</span></a>
</span><span id="TPCA-170"><a href="#TPCA-170"><span class="linenos">170</span></a><span class="sd">        Returns</span>
</span><span id="TPCA-171"><a href="#TPCA-171"><span class="linenos">171</span></a><span class="sd">        -------</span>
</span><span id="TPCA-172"><a href="#TPCA-172"><span class="linenos">172</span></a><span class="sd">        X_transformed : ndarray of shape (n, n_components)</span>
</span><span id="TPCA-173"><a href="#TPCA-173"><span class="linenos">173</span></a><span class="sd">            TCAM projections in 2D transformed space.</span>
</span><span id="TPCA-174"><a href="#TPCA-174"><span class="linenos">174</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="TPCA-175"><a href="#TPCA-175"><span class="linenos">175</span></a>
</span><span id="TPCA-176"><a href="#TPCA-176"><span class="linenos">176</span></a>        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="TPCA-177"><a href="#TPCA-177"><span class="linenos">177</span></a>        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;Ensure order-3 tensor input&quot;</span>
</span><span id="TPCA-178"><a href="#TPCA-178"><span class="linenos">178</span></a>        <span class="k">assert</span> <span class="p">(</span>
</span><span id="TPCA-179"><a href="#TPCA-179"><span class="linenos">179</span></a>            <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">p_</span> <span class="ow">and</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span>
</span><span id="TPCA-180"><a href="#TPCA-180"><span class="linenos">180</span></a>        <span class="p">),</span> <span class="s2">&quot;Ensure the number of features, and time points, matches the model fit data&quot;</span>
</span><span id="TPCA-181"><a href="#TPCA-181"><span class="linenos">181</span></a>
</span><span id="TPCA-182"><a href="#TPCA-182"><span class="linenos">182</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">centre</span><span class="p">:</span>
</span><span id="TPCA-183"><a href="#TPCA-183"><span class="linenos">183</span></a>            <span class="n">X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span>
</span><span id="TPCA-184"><a href="#TPCA-184"><span class="linenos">184</span></a>
</span><span id="TPCA-185"><a href="#TPCA-185"><span class="linenos">185</span></a>        <span class="c1"># in the interest of efficiency, V was returned in the m-transformed</span>
</span><span id="TPCA-186"><a href="#TPCA-186"><span class="linenos">186</span></a>        <span class="c1"># space from tsvdm saving a pair of roundabout calls to M and Minv,</span>
</span><span id="TPCA-187"><a href="#TPCA-187"><span class="linenos">187</span></a>        <span class="c1"># and pick out the top i_q and j_q indexes, as notated in</span>
</span><span id="TPCA-188"><a href="#TPCA-188"><span class="linenos">188</span></a>        <span class="c1"># Mor et al. (2022)</span>
</span><span id="TPCA-189"><a href="#TPCA-189"><span class="linenos">189</span></a>        <span class="k">return</span> <span class="n">facewise_product</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">M_</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hatV</span><span class="p">)[</span>
</span><span id="TPCA-190"><a href="#TPCA-190"><span class="linenos">190</span></a>            <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_k_t_flatten_sort</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_k_t_flatten_sort</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="TPCA-191"><a href="#TPCA-191"><span class="linenos">191</span></a>        <span class="p">]</span>
</span><span id="TPCA-192"><a href="#TPCA-192"><span class="linenos">192</span></a>
</span><span id="TPCA-193"><a href="#TPCA-193"><span class="linenos">193</span></a>    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span><span id="TPCA-194"><a href="#TPCA-194"><span class="linenos">194</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Fit the model with X and apply the dimensionality reduction on X.</span>
</span><span id="TPCA-195"><a href="#TPCA-195"><span class="linenos">195</span></a>
</span><span id="TPCA-196"><a href="#TPCA-196"><span class="linenos">196</span></a><span class="sd">        The output here will not be identical to calling fix(X).transform(X).</span>
</span><span id="TPCA-197"><a href="#TPCA-197"><span class="linenos">197</span></a><span class="sd">        But, the two give the same results up to machine precision. We</span>
</span><span id="TPCA-198"><a href="#TPCA-198"><span class="linenos">198</span></a><span class="sd">        provide a brief discussion of this below.</span>
</span><span id="TPCA-199"><a href="#TPCA-199"><span class="linenos">199</span></a>
</span><span id="TPCA-200"><a href="#TPCA-200"><span class="linenos">200</span></a><span class="sd">        Parameters</span>
</span><span id="TPCA-201"><a href="#TPCA-201"><span class="linenos">201</span></a><span class="sd">        ----------</span>
</span><span id="TPCA-202"><a href="#TPCA-202"><span class="linenos">202</span></a><span class="sd">        X : ndarray of shape (n, p, t)</span>
</span><span id="TPCA-203"><a href="#TPCA-203"><span class="linenos">203</span></a><span class="sd">            Training data, where `n` is the number of samples, `p` is the</span>
</span><span id="TPCA-204"><a href="#TPCA-204"><span class="linenos">204</span></a><span class="sd">            number of features, as `t` is the number of time points.</span>
</span><span id="TPCA-205"><a href="#TPCA-205"><span class="linenos">205</span></a>
</span><span id="TPCA-206"><a href="#TPCA-206"><span class="linenos">206</span></a><span class="sd">        y : Ignored</span>
</span><span id="TPCA-207"><a href="#TPCA-207"><span class="linenos">207</span></a><span class="sd">            Ignored.</span>
</span><span id="TPCA-208"><a href="#TPCA-208"><span class="linenos">208</span></a>
</span><span id="TPCA-209"><a href="#TPCA-209"><span class="linenos">209</span></a><span class="sd">        Returns</span>
</span><span id="TPCA-210"><a href="#TPCA-210"><span class="linenos">210</span></a><span class="sd">        -------</span>
</span><span id="TPCA-211"><a href="#TPCA-211"><span class="linenos">211</span></a><span class="sd">        X_transformed : ndarray of shape (n, n_components)</span>
</span><span id="TPCA-212"><a href="#TPCA-212"><span class="linenos">212</span></a><span class="sd">            TCAM projections in 2D transformed space.</span>
</span><span id="TPCA-213"><a href="#TPCA-213"><span class="linenos">213</span></a>
</span><span id="TPCA-214"><a href="#TPCA-214"><span class="linenos">214</span></a><span class="sd">        Notes</span>
</span><span id="TPCA-215"><a href="#TPCA-215"><span class="linenos">215</span></a><span class="sd">        -----</span>
</span><span id="TPCA-216"><a href="#TPCA-216"><span class="linenos">216</span></a><span class="sd">        The tensor m-product from Kilmer et al. (2021) has a notion of tensor</span>
</span><span id="TPCA-217"><a href="#TPCA-217"><span class="linenos">217</span></a><span class="sd">        inverse, and tensor orthogonality.</span>
</span><span id="TPCA-218"><a href="#TPCA-218"><span class="linenos">218</span></a>
</span><span id="TPCA-219"><a href="#TPCA-219"><span class="linenos">219</span></a><span class="sd">        We benchmarked an alternative approach as taken by sklearn in their</span>
</span><span id="TPCA-220"><a href="#TPCA-220"><span class="linenos">220</span></a><span class="sd">        PCA class. If we right multiply A&#39;s tSVDM by $V$ we note that it</span>
</span><span id="TPCA-221"><a href="#TPCA-221"><span class="linenos">221</span></a><span class="sd">        cancels the $V^T$ giving us:</span>
</span><span id="TPCA-222"><a href="#TPCA-222"><span class="linenos">222</span></a><span class="sd">            $$</span>
</span><span id="TPCA-223"><a href="#TPCA-223"><span class="linenos">223</span></a><span class="sd">                Z = A *_M V = U *_M S</span>
</span><span id="TPCA-224"><a href="#TPCA-224"><span class="linenos">224</span></a><span class="sd">            $$</span>
</span><span id="TPCA-225"><a href="#TPCA-225"><span class="linenos">225</span></a><span class="sd">        It appears that computing the final term is more computationally</span>
</span><span id="TPCA-226"><a href="#TPCA-226"><span class="linenos">226</span></a><span class="sd">        efficient, even if we have to convert $S$ into its full (sparse)</span>
</span><span id="TPCA-227"><a href="#TPCA-227"><span class="linenos">227</span></a><span class="sd">        tensor representation.</span>
</span><span id="TPCA-228"><a href="#TPCA-228"><span class="linenos">228</span></a>
</span><span id="TPCA-229"><a href="#TPCA-229"><span class="linenos">229</span></a><span class="sd">        By contrast, transform(X) will simply compute</span>
</span><span id="TPCA-230"><a href="#TPCA-230"><span class="linenos">230</span></a><span class="sd">            $$</span>
</span><span id="TPCA-231"><a href="#TPCA-231"><span class="linenos">231</span></a><span class="sd">                Z = A *_M V</span>
</span><span id="TPCA-232"><a href="#TPCA-232"><span class="linenos">232</span></a><span class="sd">            $$</span>
</span><span id="TPCA-233"><a href="#TPCA-233"><span class="linenos">233</span></a>
</span><span id="TPCA-234"><a href="#TPCA-234"><span class="linenos">234</span></a><span class="sd">        In both cases, $Z$ still needs to be converted by $\\times_3 M^{-1}$</span>
</span><span id="TPCA-235"><a href="#TPCA-235"><span class="linenos">235</span></a><span class="sd">        and &#39;compressed&#39; before being returned.</span>
</span><span id="TPCA-236"><a href="#TPCA-236"><span class="linenos">236</span></a><span class="sd">        For these details see Mor et al. (2022).</span>
</span><span id="TPCA-237"><a href="#TPCA-237"><span class="linenos">237</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="TPCA-238"><a href="#TPCA-238"><span class="linenos">238</span></a>        <span class="c1"># note that these tensors do NOT have full face-wise matrices</span>
</span><span id="TPCA-239"><a href="#TPCA-239"><span class="linenos">239</span></a>        <span class="n">hatU</span><span class="p">,</span> <span class="n">hatS_mat</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="TPCA-240"><a href="#TPCA-240"><span class="linenos">240</span></a>        <span class="n">hatS</span> <span class="o">=</span> <span class="n">_singular_vals_mat_to_tensor</span><span class="p">(</span><span class="n">hatS_mat</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span><span class="p">)</span>
</span><span id="TPCA-241"><a href="#TPCA-241"><span class="linenos">241</span></a>        <span class="k">return</span> <span class="n">facewise_product</span><span class="p">(</span><span class="n">hatU</span><span class="p">,</span> <span class="n">hatS</span><span class="p">)[</span>
</span><span id="TPCA-242"><a href="#TPCA-242"><span class="linenos">242</span></a>            <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_k_t_flatten_sort</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_k_t_flatten_sort</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="TPCA-243"><a href="#TPCA-243"><span class="linenos">243</span></a>        <span class="p">]</span>
</span><span id="TPCA-244"><a href="#TPCA-244"><span class="linenos">244</span></a>
</span><span id="TPCA-245"><a href="#TPCA-245"><span class="linenos">245</span></a>    <span class="k">def</span> <span class="nf">_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="TPCA-246"><a href="#TPCA-246"><span class="linenos">246</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Fit the model by computing the full SVD on X. Implementation</span>
</span><span id="TPCA-247"><a href="#TPCA-247"><span class="linenos">247</span></a><span class="sd">        loosely modelled around `_fit_full()` method from sklearn&#39;s PCA.</span>
</span><span id="TPCA-248"><a href="#TPCA-248"><span class="linenos">248</span></a><span class="sd">        In the future, we could potentially explore different svd solvers</span>
</span><span id="TPCA-249"><a href="#TPCA-249"><span class="linenos">249</span></a><span class="sd">        which lets one directly pass truncation specifications into the</span>
</span><span id="TPCA-250"><a href="#TPCA-250"><span class="linenos">250</span></a><span class="sd">        low-level solver...?</span>
</span><span id="TPCA-251"><a href="#TPCA-251"><span class="linenos">251</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="TPCA-252"><a href="#TPCA-252"><span class="linenos">252</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span>
</span><span id="TPCA-253"><a href="#TPCA-253"><span class="linenos">253</span></a>            <span class="nb">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">)</span> <span class="o">^</span> <span class="nb">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Minv</span><span class="p">)</span>
</span><span id="TPCA-254"><a href="#TPCA-254"><span class="linenos">254</span></a>        <span class="p">),</span> <span class="s2">&quot;If explicitly defined, both M and its inverse must be defined&quot;</span>
</span><span id="TPCA-255"><a href="#TPCA-255"><span class="linenos">255</span></a>
</span><span id="TPCA-256"><a href="#TPCA-256"><span class="linenos">256</span></a>        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;Ensure order-3 tensor input&quot;</span>
</span><span id="TPCA-257"><a href="#TPCA-257"><span class="linenos">257</span></a>        <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</span><span id="TPCA-258"><a href="#TPCA-258"><span class="linenos">258</span></a>        <span class="n">k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</span><span id="TPCA-259"><a href="#TPCA-259"><span class="linenos">259</span></a>
</span><span id="TPCA-260"><a href="#TPCA-260"><span class="linenos">260</span></a>        <span class="c1"># center the data into mean deviation form, see Mor et al. (2022)</span>
</span><span id="TPCA-261"><a href="#TPCA-261"><span class="linenos">261</span></a>        <span class="c1"># similar to sklearns PCA, we choose to implement this within the</span>
</span><span id="TPCA-262"><a href="#TPCA-262"><span class="linenos">262</span></a>        <span class="c1"># class and just store the mean slice for subsequent transform calls</span>
</span><span id="TPCA-263"><a href="#TPCA-263"><span class="linenos">263</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="TPCA-264"><a href="#TPCA-264"><span class="linenos">264</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy</span><span class="p">:</span>
</span><span id="TPCA-265"><a href="#TPCA-265"><span class="linenos">265</span></a>            <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span><span id="TPCA-266"><a href="#TPCA-266"><span class="linenos">266</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">centre</span><span class="p">:</span>
</span><span id="TPCA-267"><a href="#TPCA-267"><span class="linenos">267</span></a>            <span class="n">X</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span>
</span><span id="TPCA-268"><a href="#TPCA-268"><span class="linenos">268</span></a>
</span><span id="TPCA-269"><a href="#TPCA-269"><span class="linenos">269</span></a>        <span class="c1"># if there is no explicitly defined transform in __init__, assign</span>
</span><span id="TPCA-270"><a href="#TPCA-270"><span class="linenos">270</span></a>        <span class="c1"># functions to perform a default transformation</span>
</span><span id="TPCA-271"><a href="#TPCA-271"><span class="linenos">271</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="nb">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">):</span>
</span><span id="TPCA-272"><a href="#TPCA-272"><span class="linenos">272</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">M_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Minv_</span> <span class="o">=</span> <span class="n">generate_default_m_transform_pair</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</span><span id="TPCA-273"><a href="#TPCA-273"><span class="linenos">273</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="TPCA-274"><a href="#TPCA-274"><span class="linenos">274</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">M_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Minv_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Minv</span>
</span><span id="TPCA-275"><a href="#TPCA-275"><span class="linenos">275</span></a>
</span><span id="TPCA-276"><a href="#TPCA-276"><span class="linenos">276</span></a>        <span class="c1"># perform tensor decomposition via Kilmer&#39;s tSVDM</span>
</span><span id="TPCA-277"><a href="#TPCA-277"><span class="linenos">277</span></a>        <span class="n">hatU</span><span class="p">,</span> <span class="n">hatS_mat</span><span class="p">,</span> <span class="n">hatV</span> <span class="o">=</span> <span class="n">tsvdm</span><span class="p">(</span>
</span><span id="TPCA-278"><a href="#TPCA-278"><span class="linenos">278</span></a>            <span class="n">X</span><span class="p">,</span>
</span><span id="TPCA-279"><a href="#TPCA-279"><span class="linenos">279</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">M_</span><span class="p">,</span>
</span><span id="TPCA-280"><a href="#TPCA-280"><span class="linenos">280</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">Minv_</span><span class="p">,</span>
</span><span id="TPCA-281"><a href="#TPCA-281"><span class="linenos">281</span></a>            <span class="n">keep_hats</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="TPCA-282"><a href="#TPCA-282"><span class="linenos">282</span></a>            <span class="n">full_frontal_slices</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="TPCA-283"><a href="#TPCA-283"><span class="linenos">283</span></a>            <span class="n">svals_matrix_form</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="TPCA-284"><a href="#TPCA-284"><span class="linenos">284</span></a>        <span class="p">)</span>
</span><span id="TPCA-285"><a href="#TPCA-285"><span class="linenos">285</span></a>
</span><span id="TPCA-286"><a href="#TPCA-286"><span class="linenos">286</span></a>        <span class="c1"># we flatten out the compressed singular value matrix in Fortran memory</span>
</span><span id="TPCA-287"><a href="#TPCA-287"><span class="linenos">287</span></a>        <span class="c1"># style (column-wise). tensor-wise, we can interpret this as stacking</span>
</span><span id="TPCA-288"><a href="#TPCA-288"><span class="linenos">288</span></a>        <span class="c1"># the diagonals of each tensor face in S next to each other in the</span>
</span><span id="TPCA-289"><a href="#TPCA-289"><span class="linenos">289</span></a>        <span class="c1"># flattened array, where the singular values are grouped by face</span>
</span><span id="TPCA-290"><a href="#TPCA-290"><span class="linenos">290</span></a>        <span class="n">singular_values_</span> <span class="o">=</span> <span class="n">hatS_mat</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>
</span><span id="TPCA-291"><a href="#TPCA-291"><span class="linenos">291</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_k_t_flatten_sort</span> <span class="o">=</span> <span class="n">singular_values_</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="TPCA-292"><a href="#TPCA-292"><span class="linenos">292</span></a>        <span class="n">singular_values_</span> <span class="o">=</span> <span class="n">singular_values_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_k_t_flatten_sort</span><span class="p">]</span>
</span><span id="TPCA-293"><a href="#TPCA-293"><span class="linenos">293</span></a>
</span><span id="TPCA-294"><a href="#TPCA-294"><span class="linenos">294</span></a>        <span class="c1"># get variance explained by singular values</span>
</span><span id="TPCA-295"><a href="#TPCA-295"><span class="linenos">295</span></a>        <span class="c1"># note that we are not yet aware of any notion of &#39;variance&#39; for random</span>
</span><span id="TPCA-296"><a href="#TPCA-296"><span class="linenos">296</span></a>        <span class="c1"># tensors so we do not have sklearn PCA&#39;s self.explained_variance_</span>
</span><span id="TPCA-297"><a href="#TPCA-297"><span class="linenos">297</span></a>        <span class="c1"># however we may find literature for this in the future to include it</span>
</span><span id="TPCA-298"><a href="#TPCA-298"><span class="linenos">298</span></a>        <span class="n">squared_singular_values</span> <span class="o">=</span> <span class="n">singular_values_</span><span class="o">**</span><span class="mi">2</span>
</span><span id="TPCA-299"><a href="#TPCA-299"><span class="linenos">299</span></a>        <span class="n">total_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">squared_singular_values</span><span class="p">)</span>
</span><span id="TPCA-300"><a href="#TPCA-300"><span class="linenos">300</span></a>        <span class="n">explained_variance_ratio_</span> <span class="o">=</span> <span class="n">squared_singular_values</span> <span class="o">/</span> <span class="n">total_var</span>
</span><span id="TPCA-301"><a href="#TPCA-301"><span class="linenos">301</span></a>
</span><span id="TPCA-302"><a href="#TPCA-302"><span class="linenos">302</span></a>        <span class="c1"># process n_components input</span>
</span><span id="TPCA-303"><a href="#TPCA-303"><span class="linenos">303</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="TPCA-304"><a href="#TPCA-304"><span class="linenos">304</span></a>            <span class="n">n_components</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">t</span>
</span><span id="TPCA-305"><a href="#TPCA-305"><span class="linenos">305</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span> <span class="n">RealNotInt</span><span class="p">):</span>
</span><span id="TPCA-306"><a href="#TPCA-306"><span class="linenos">306</span></a>            <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
</span><span id="TPCA-307"><a href="#TPCA-307"><span class="linenos">307</span></a>                <span class="c1"># retrieve the integer number of components required to explain</span>
</span><span id="TPCA-308"><a href="#TPCA-308"><span class="linenos">308</span></a>                <span class="c1"># this proportion of the total squared sum of singular values</span>
</span><span id="TPCA-309"><a href="#TPCA-309"><span class="linenos">309</span></a>                <span class="n">ratio_cumsum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>
</span><span id="TPCA-310"><a href="#TPCA-310"><span class="linenos">310</span></a>                <span class="c1"># np.searchsorted call returns the mininum i</span>
</span><span id="TPCA-311"><a href="#TPCA-311"><span class="linenos">311</span></a>                <span class="c1">#   s.t. n_components &lt; ratio_cumsum[i] (see numpy docs)</span>
</span><span id="TPCA-312"><a href="#TPCA-312"><span class="linenos">312</span></a>                <span class="c1"># which means that the (i+1)th element in ratio_cumsum[i]</span>
</span><span id="TPCA-313"><a href="#TPCA-313"><span class="linenos">313</span></a>                <span class="c1"># strictly exceeds the user&#39;s specified variance ratio</span>
</span><span id="TPCA-314"><a href="#TPCA-314"><span class="linenos">314</span></a>                <span class="n">n_components</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="TPCA-315"><a href="#TPCA-315"><span class="linenos">315</span></a>                    <span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">ratio_cumsum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span> <span class="n">side</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="TPCA-316"><a href="#TPCA-316"><span class="linenos">316</span></a>                <span class="p">)</span>
</span><span id="TPCA-317"><a href="#TPCA-317"><span class="linenos">317</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="TPCA-318"><a href="#TPCA-318"><span class="linenos">318</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="TPCA-319"><a href="#TPCA-319"><span class="linenos">319</span></a>                    <span class="s2">&quot;For non-integer inputs, ensure that 0 &lt; n_components &lt; 1&quot;</span>
</span><span id="TPCA-320"><a href="#TPCA-320"><span class="linenos">320</span></a>                <span class="p">)</span>
</span><span id="TPCA-321"><a href="#TPCA-321"><span class="linenos">321</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span> <span class="n">Integral</span><span class="p">):</span>
</span><span id="TPCA-322"><a href="#TPCA-322"><span class="linenos">322</span></a>            <span class="k">if</span> <span class="mi">1</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="o">&lt;=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">t</span><span class="p">:</span>
</span><span id="TPCA-323"><a href="#TPCA-323"><span class="linenos">323</span></a>                <span class="n">n_components</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span>
</span><span id="TPCA-324"><a href="#TPCA-324"><span class="linenos">324</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="TPCA-325"><a href="#TPCA-325"><span class="linenos">325</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="TPCA-326"><a href="#TPCA-326"><span class="linenos">326</span></a>                    <span class="sa">f</span><span class="s2">&quot;Integer inputs must satisfy 1 &lt;= n_components &lt;= min(n, p)*t=</span><span class="si">{</span><span class="n">k</span><span class="o">*</span><span class="n">t</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="TPCA-327"><a href="#TPCA-327"><span class="linenos">327</span></a>                <span class="p">)</span>
</span><span id="TPCA-328"><a href="#TPCA-328"><span class="linenos">328</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="TPCA-329"><a href="#TPCA-329"><span class="linenos">329</span></a>            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
</span><span id="TPCA-330"><a href="#TPCA-330"><span class="linenos">330</span></a>                <span class="s2">&quot;n_components must be an integer, float, or None&quot;</span>
</span><span id="TPCA-331"><a href="#TPCA-331"><span class="linenos">331</span></a>                <span class="sa">f</span><span class="s2">&quot;Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">)</span><span class="si">}</span><span class="s2"> instead&quot;</span>
</span><span id="TPCA-332"><a href="#TPCA-332"><span class="linenos">332</span></a>            <span class="p">)</span>
</span><span id="TPCA-333"><a href="#TPCA-333"><span class="linenos">333</span></a>
</span><span id="TPCA-334"><a href="#TPCA-334"><span class="linenos">334</span></a>        <span class="c1"># convert the argsort indexes back into the two dimensional indexes.</span>
</span><span id="TPCA-335"><a href="#TPCA-335"><span class="linenos">335</span></a>        <span class="c1"># in the same tensor semantics as hatS_mat, the rows (tuple[0]) in this</span>
</span><span id="TPCA-336"><a href="#TPCA-336"><span class="linenos">336</span></a>        <span class="c1"># multindex correspond to the p-dimension location, and the</span>
</span><span id="TPCA-337"><a href="#TPCA-337"><span class="linenos">337</span></a>        <span class="c1"># columns (tuple[1]) in the multindex correspond to the t-dimension</span>
</span><span id="TPCA-338"><a href="#TPCA-338"><span class="linenos">338</span></a>        <span class="c1"># location. these are now the collection of i_h&#39;s and j_h&#39;s</span>
</span><span id="TPCA-339"><a href="#TPCA-339"><span class="linenos">339</span></a>        <span class="c1"># in Mor et al. (2022)</span>
</span><span id="TPCA-340"><a href="#TPCA-340"><span class="linenos">340</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_k_t_flatten_sort</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unravel_index</span><span class="p">(</span>
</span><span id="TPCA-341"><a href="#TPCA-341"><span class="linenos">341</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_k_t_flatten_sort</span><span class="p">[:</span><span class="n">n_components</span><span class="p">],</span> <span class="n">shape</span><span class="o">=</span><span class="n">hatS_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span>
</span><span id="TPCA-342"><a href="#TPCA-342"><span class="linenos">342</span></a>        <span class="p">)</span>
</span><span id="TPCA-343"><a href="#TPCA-343"><span class="linenos">343</span></a>
</span><span id="TPCA-344"><a href="#TPCA-344"><span class="linenos">344</span></a>        <span class="c1"># perform truncation. this speeds up subsequent calls to transform,</span>
</span><span id="TPCA-345"><a href="#TPCA-345"><span class="linenos">345</span></a>        <span class="c1"># but is not required to obtain the correct results. we include it</span>
</span><span id="TPCA-346"><a href="#TPCA-346"><span class="linenos">346</span></a>        <span class="c1"># because it also computes rho, at basically no extra cost</span>
</span><span id="TPCA-347"><a href="#TPCA-347"><span class="linenos">347</span></a>        <span class="n">rho</span> <span class="o">=</span> <span class="n">_rank_q_truncation_zero_out</span><span class="p">(</span>
</span><span id="TPCA-348"><a href="#TPCA-348"><span class="linenos">348</span></a>            <span class="n">hatU</span><span class="p">,</span> <span class="n">hatS_mat</span><span class="p">,</span> <span class="n">hatV</span><span class="p">,</span> <span class="n">sigma_q</span><span class="o">=</span><span class="n">singular_values_</span><span class="p">[</span><span class="n">n_components</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
</span><span id="TPCA-349"><a href="#TPCA-349"><span class="linenos">349</span></a>        <span class="p">)</span>
</span><span id="TPCA-350"><a href="#TPCA-350"><span class="linenos">350</span></a>
</span><span id="TPCA-351"><a href="#TPCA-351"><span class="linenos">351</span></a>        <span class="c1"># we store the features tensor, in the tSVDM decomposition, in the</span>
</span><span id="TPCA-352"><a href="#TPCA-352"><span class="linenos">352</span></a>        <span class="c1"># transforemd space, saving roundabout calls to M and Minv when</span>
</span><span id="TPCA-353"><a href="#TPCA-353"><span class="linenos">353</span></a>        <span class="c1"># performing m-product with new data</span>
</span><span id="TPCA-354"><a href="#TPCA-354"><span class="linenos">354</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_hatV</span> <span class="o">=</span> <span class="n">hatV</span>
</span><span id="TPCA-355"><a href="#TPCA-355"><span class="linenos">355</span></a>
</span><span id="TPCA-356"><a href="#TPCA-356"><span class="linenos">356</span></a>        <span class="c1"># store public attributes; as per sklearn conventions, we use trailing</span>
</span><span id="TPCA-357"><a href="#TPCA-357"><span class="linenos">357</span></a>        <span class="c1"># underscores to indicate that they have been populated following a</span>
</span><span id="TPCA-358"><a href="#TPCA-358"><span class="linenos">358</span></a>        <span class="c1"># call to fit()</span>
</span><span id="TPCA-359"><a href="#TPCA-359"><span class="linenos">359</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">n_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">p_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_</span> <span class="o">=</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">k</span>
</span><span id="TPCA-360"><a href="#TPCA-360"><span class="linenos">360</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">n_components_</span> <span class="o">=</span> <span class="n">n_components</span>
</span><span id="TPCA-361"><a href="#TPCA-361"><span class="linenos">361</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">explained_variance_ratio_</span> <span class="o">=</span> <span class="n">explained_variance_ratio_</span><span class="p">[:</span><span class="n">n_components</span><span class="p">]</span>
</span><span id="TPCA-362"><a href="#TPCA-362"><span class="linenos">362</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">singular_values_</span> <span class="o">=</span> <span class="n">singular_values_</span><span class="p">[:</span><span class="n">n_components</span><span class="p">]</span>
</span><span id="TPCA-363"><a href="#TPCA-363"><span class="linenos">363</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">rho_</span> <span class="o">=</span> <span class="n">rho</span>
</span><span id="TPCA-364"><a href="#TPCA-364"><span class="linenos">364</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">loadings_matrix_</span> <span class="o">=</span> <span class="n">hatV</span><span class="p">[</span>
</span><span id="TPCA-365"><a href="#TPCA-365"><span class="linenos">365</span></a>            <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_k_t_flatten_sort</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_k_t_flatten_sort</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="TPCA-366"><a href="#TPCA-366"><span class="linenos">366</span></a>        <span class="p">]</span><span class="o">.</span><span class="n">T</span>
</span><span id="TPCA-367"><a href="#TPCA-367"><span class="linenos">367</span></a>
</span><span id="TPCA-368"><a href="#TPCA-368"><span class="linenos">368</span></a>        <span class="k">return</span> <span class="n">hatU</span><span class="p">,</span> <span class="n">hatS_mat</span><span class="p">,</span> <span class="n">hatV</span>
</span><span id="TPCA-369"><a href="#TPCA-369"><span class="linenos">369</span></a>
</span><span id="TPCA-370"><a href="#TPCA-370"><span class="linenos">370</span></a>    <span class="nd">@property</span>
</span><span id="TPCA-371"><a href="#TPCA-371"><span class="linenos">371</span></a>    <span class="k">def</span> <span class="nf">_n_features_out</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="TPCA-372"><a href="#TPCA-372"><span class="linenos">372</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of transformed output features.</span>
</span><span id="TPCA-373"><a href="#TPCA-373"><span class="linenos">373</span></a>
</span><span id="TPCA-374"><a href="#TPCA-374"><span class="linenos">374</span></a><span class="sd">        [CAN IGNORE - NOT MATHEMATICALLY RELEVANT]:</span>
</span><span id="TPCA-375"><a href="#TPCA-375"><span class="linenos">375</span></a><span class="sd">        See sklearn/decompositions/_base.py</span>
</span><span id="TPCA-376"><a href="#TPCA-376"><span class="linenos">376</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="TPCA-377"><a href="#TPCA-377"><span class="linenos">377</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components_</span>
</span></pre></div>


            <div class="docstring"><p>Tensor analogue of PCA, introduced by Mor et al. (2022).</p>

<p>t-SVDM tensor analogue of PCA using explicit rank truncation with explicit
rank truncation from Mor et al. (2022), and underlying m-product framework
from Kilmer et al. (2021). Takes in an $n \times p \times t$ input
tensor, and transforms into a $n \times$ <code>n_components</code> matrix of 2D
transformed projections.</p>

<p>The input tensor is centred into Mean Deviation Form (by location), but
not normalized (by scale).</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><p><strong>n_components</strong> (int, float, or None, default=None):
Control number of components to keep. If n_components is not set at
all, or set to <code>None</code>, <code>n_components == min(n, p) * t</code></p>

<p>If <code>n_components</code> is an integer, the TPCA will return the number of
loadings, provided that for the tensor data passed into <code><a href="#TPCA.fit">fit</a></code>,
satisfies <code>1 &lt;= n_components &lt;= min(n, p) * t</code></p>

<p>If <code>0 &lt; n_components &lt; 1</code>, TPCA will select the number of
compononents such that the amount of variance that needs to be
explained is greater than the percentage specified.</p></li>
<li><strong>copy</strong> (bool, default=True):
If False, data passed to fit are overwritten and running
<code>fit(X).transform(X)</code> will not yield the expected results. Use
<code>fit_transform(X)</code> instead.</li>
<li><strong>M</strong> (Callable[[ndarray], ndarray] or None, default=None):
A function which expects an order-3 tensor as input, and returns the
image under a m-transform. If unspecified TPCA will use the Discrete
Cosine Transform (ii) from <code>scipy.fft</code>.</li>
<li><strong>Minv</strong> (Callable[[ndarray], ndarray] or None, default=None):
A function implementing the inverse transform of <code>M</code>.</li>
<li><strong>centre</strong> (bool, default=True):
If False, the data tensor will not be centralized into Mean Deviation
Form. By default, the mean horizontal slice of the tensor is
subtracted, so that all of the horizontal slices sum to 0, analagous
to centering the data in PCA.</li>
</ul>

<h6 id="attributes">Attributes</h6>

<ul>
<li><strong>n_, p_, t_, k_</strong> (int):
The dimensions of the training data. <code>k_ == min(n_, p_)</code></li>
<li><strong>M_, MInv_</strong> (Callable[[ndarray], ndarray]):
The m-transform pair (forward and inverse) used for the underlying
tensor-tensor m-product.</li>
<li><strong>n_components_</strong> (int):
The estimated number of components. If <code>n_components</code> was explicitly
set by an integer value, this will be the same as that. If
<code>n_components</code> was a number between 0 and 1, this number is estimated
from input data. Otherwise, if not set (defaults to None), it will
default to $k \times t$ in the training data.</li>
<li><p><strong>explained_variance_ratio_</strong> (ndarray of shape (n_components_,)):
Percentage of total variance explained by each of the selected
components. The selected components are selected so that this is
returned in descending order.</p>

<p>If <code>n_components</code> is not set then all components are stored and
the sum of this ratios array is equal to 1.0.</p></li>
<li><strong>singular_values_</strong> (ndarray of shape (n_components,)):
The singular values corresponding to each of the selected components.</li>
<li><strong>mean_</strong> (ndarray of shape (p_, t_)):
Per-feature, per-timepoint empirical mean, estimated from the
training set. This is used to normalize any new data passed to
<code>transform(X)</code>, unless centre is explicitly turned off via
<code>centre==False</code> during object instantiation.</li>
<li><strong>rho_</strong> (ndarray of shape (t,)):
The rho used in multi-rank truncation to achieve the desired explicit
rank of <code>n_components</code>. See Mor et al. (2022) for detail.</li>
<li><strong>loadings_matrix_</strong> (ndarray of shape (n_components_, p_)):
The i-th row corresponds to the column of $\hat{V}$ which contains
the feature weights applied to the data (in the hat-space) to get the
i-th TPCA component.</li>
</ul>

<h6 id="references">References</h6>

<p>Mor, U., Cohen, Y., Valdés-Mas, R., Kviatcovsky, D., Elinav, E. and Avron,
H., 2022. Dimensionality reduction of longitudinal’omics data using modern
tensor factorizations. PLoS Computational Biology, 18(7), p.e1010212.</p>
</div>


                            <div id="TPCA.fit" class="classattr">
                                        <input id="TPCA.fit-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">fit</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">X</span>, </span><span class="param"><span class="n">y</span><span class="o">=</span><span class="kc">None</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="TPCA.fit-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#TPCA.fit"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="TPCA.fit-136"><a href="#TPCA.fit-136"><span class="linenos">136</span></a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span><span id="TPCA.fit-137"><a href="#TPCA.fit-137"><span class="linenos">137</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Fit the model with X.</span>
</span><span id="TPCA.fit-138"><a href="#TPCA.fit-138"><span class="linenos">138</span></a>
</span><span id="TPCA.fit-139"><a href="#TPCA.fit-139"><span class="linenos">139</span></a><span class="sd">        Parameters</span>
</span><span id="TPCA.fit-140"><a href="#TPCA.fit-140"><span class="linenos">140</span></a><span class="sd">        ----------</span>
</span><span id="TPCA.fit-141"><a href="#TPCA.fit-141"><span class="linenos">141</span></a><span class="sd">        X : ndarray of shape (n, p, t)</span>
</span><span id="TPCA.fit-142"><a href="#TPCA.fit-142"><span class="linenos">142</span></a><span class="sd">            Training data, where `n` is the number of samples, `p` is the</span>
</span><span id="TPCA.fit-143"><a href="#TPCA.fit-143"><span class="linenos">143</span></a><span class="sd">            number of features, as `t` is the number of time points.</span>
</span><span id="TPCA.fit-144"><a href="#TPCA.fit-144"><span class="linenos">144</span></a>
</span><span id="TPCA.fit-145"><a href="#TPCA.fit-145"><span class="linenos">145</span></a><span class="sd">        y : Ignored</span>
</span><span id="TPCA.fit-146"><a href="#TPCA.fit-146"><span class="linenos">146</span></a><span class="sd">            Ignored.</span>
</span><span id="TPCA.fit-147"><a href="#TPCA.fit-147"><span class="linenos">147</span></a>
</span><span id="TPCA.fit-148"><a href="#TPCA.fit-148"><span class="linenos">148</span></a><span class="sd">        Returns</span>
</span><span id="TPCA.fit-149"><a href="#TPCA.fit-149"><span class="linenos">149</span></a><span class="sd">        -------</span>
</span><span id="TPCA.fit-150"><a href="#TPCA.fit-150"><span class="linenos">150</span></a><span class="sd">        self : object</span>
</span><span id="TPCA.fit-151"><a href="#TPCA.fit-151"><span class="linenos">151</span></a><span class="sd">            Returns the instance itself, after being fitted.</span>
</span><span id="TPCA.fit-152"><a href="#TPCA.fit-152"><span class="linenos">152</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="TPCA.fit-153"><a href="#TPCA.fit-153"><span class="linenos">153</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="TPCA.fit-154"><a href="#TPCA.fit-154"><span class="linenos">154</span></a>        <span class="k">return</span> <span class="bp">self</span>
</span></pre></div>


            <div class="docstring"><p>Fit the model with X.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>X</strong> (ndarray of shape (n, p, t)):
Training data, where <code>n</code> is the number of samples, <code>p</code> is the
number of features, as <code>t</code> is the number of time points.</li>
<li><strong>y</strong> (Ignored):
Ignored.</li>
</ul>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>self</strong> (object):
Returns the instance itself, after being fitted.</li>
</ul>
</div>


                            </div>
                            <div id="TPCA.transform" class="classattr">
                                        <input id="TPCA.transform-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">transform</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">X</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="TPCA.transform-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#TPCA.transform"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="TPCA.transform-156"><a href="#TPCA.transform-156"><span class="linenos">156</span></a>    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="TPCA.transform-157"><a href="#TPCA.transform-157"><span class="linenos">157</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply dimensionality reduction to X.</span>
</span><span id="TPCA.transform-158"><a href="#TPCA.transform-158"><span class="linenos">158</span></a>
</span><span id="TPCA.transform-159"><a href="#TPCA.transform-159"><span class="linenos">159</span></a><span class="sd">        See the TCAM algorithm from Mor et al. (2022)</span>
</span><span id="TPCA.transform-160"><a href="#TPCA.transform-160"><span class="linenos">160</span></a>
</span><span id="TPCA.transform-161"><a href="#TPCA.transform-161"><span class="linenos">161</span></a><span class="sd">        Parameters</span>
</span><span id="TPCA.transform-162"><a href="#TPCA.transform-162"><span class="linenos">162</span></a><span class="sd">        ----------</span>
</span><span id="TPCA.transform-163"><a href="#TPCA.transform-163"><span class="linenos">163</span></a><span class="sd">        X : ndarray of shape (n, p, t)</span>
</span><span id="TPCA.transform-164"><a href="#TPCA.transform-164"><span class="linenos">164</span></a><span class="sd">            Training data, where `n` is the number of samples, `p` is the</span>
</span><span id="TPCA.transform-165"><a href="#TPCA.transform-165"><span class="linenos">165</span></a><span class="sd">            number of features, as `t` is the number of time points.</span>
</span><span id="TPCA.transform-166"><a href="#TPCA.transform-166"><span class="linenos">166</span></a>
</span><span id="TPCA.transform-167"><a href="#TPCA.transform-167"><span class="linenos">167</span></a><span class="sd">        y : Ignored</span>
</span><span id="TPCA.transform-168"><a href="#TPCA.transform-168"><span class="linenos">168</span></a><span class="sd">            Ignored.</span>
</span><span id="TPCA.transform-169"><a href="#TPCA.transform-169"><span class="linenos">169</span></a>
</span><span id="TPCA.transform-170"><a href="#TPCA.transform-170"><span class="linenos">170</span></a><span class="sd">        Returns</span>
</span><span id="TPCA.transform-171"><a href="#TPCA.transform-171"><span class="linenos">171</span></a><span class="sd">        -------</span>
</span><span id="TPCA.transform-172"><a href="#TPCA.transform-172"><span class="linenos">172</span></a><span class="sd">        X_transformed : ndarray of shape (n, n_components)</span>
</span><span id="TPCA.transform-173"><a href="#TPCA.transform-173"><span class="linenos">173</span></a><span class="sd">            TCAM projections in 2D transformed space.</span>
</span><span id="TPCA.transform-174"><a href="#TPCA.transform-174"><span class="linenos">174</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="TPCA.transform-175"><a href="#TPCA.transform-175"><span class="linenos">175</span></a>
</span><span id="TPCA.transform-176"><a href="#TPCA.transform-176"><span class="linenos">176</span></a>        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span><span id="TPCA.transform-177"><a href="#TPCA.transform-177"><span class="linenos">177</span></a>        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;Ensure order-3 tensor input&quot;</span>
</span><span id="TPCA.transform-178"><a href="#TPCA.transform-178"><span class="linenos">178</span></a>        <span class="k">assert</span> <span class="p">(</span>
</span><span id="TPCA.transform-179"><a href="#TPCA.transform-179"><span class="linenos">179</span></a>            <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">p_</span> <span class="ow">and</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span>
</span><span id="TPCA.transform-180"><a href="#TPCA.transform-180"><span class="linenos">180</span></a>        <span class="p">),</span> <span class="s2">&quot;Ensure the number of features, and time points, matches the model fit data&quot;</span>
</span><span id="TPCA.transform-181"><a href="#TPCA.transform-181"><span class="linenos">181</span></a>
</span><span id="TPCA.transform-182"><a href="#TPCA.transform-182"><span class="linenos">182</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">centre</span><span class="p">:</span>
</span><span id="TPCA.transform-183"><a href="#TPCA.transform-183"><span class="linenos">183</span></a>            <span class="n">X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span>
</span><span id="TPCA.transform-184"><a href="#TPCA.transform-184"><span class="linenos">184</span></a>
</span><span id="TPCA.transform-185"><a href="#TPCA.transform-185"><span class="linenos">185</span></a>        <span class="c1"># in the interest of efficiency, V was returned in the m-transformed</span>
</span><span id="TPCA.transform-186"><a href="#TPCA.transform-186"><span class="linenos">186</span></a>        <span class="c1"># space from tsvdm saving a pair of roundabout calls to M and Minv,</span>
</span><span id="TPCA.transform-187"><a href="#TPCA.transform-187"><span class="linenos">187</span></a>        <span class="c1"># and pick out the top i_q and j_q indexes, as notated in</span>
</span><span id="TPCA.transform-188"><a href="#TPCA.transform-188"><span class="linenos">188</span></a>        <span class="c1"># Mor et al. (2022)</span>
</span><span id="TPCA.transform-189"><a href="#TPCA.transform-189"><span class="linenos">189</span></a>        <span class="k">return</span> <span class="n">facewise_product</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">M_</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hatV</span><span class="p">)[</span>
</span><span id="TPCA.transform-190"><a href="#TPCA.transform-190"><span class="linenos">190</span></a>            <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_k_t_flatten_sort</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_k_t_flatten_sort</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="TPCA.transform-191"><a href="#TPCA.transform-191"><span class="linenos">191</span></a>        <span class="p">]</span>
</span></pre></div>


            <div class="docstring"><p>Apply dimensionality reduction to X.</p>

<p>See the TCAM algorithm from Mor et al. (2022)</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>X</strong> (ndarray of shape (n, p, t)):
Training data, where <code>n</code> is the number of samples, <code>p</code> is the
number of features, as <code>t</code> is the number of time points.</li>
<li><strong>y</strong> (Ignored):
Ignored.</li>
</ul>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>X_transformed</strong> (ndarray of shape (n, n_components)):
TCAM projections in 2D transformed space.</li>
</ul>
</div>


                            </div>
                            <div id="TPCA.fit_transform" class="classattr">
                                        <input id="TPCA.fit_transform-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">fit_transform</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">X</span>, </span><span class="param"><span class="n">y</span><span class="o">=</span><span class="kc">None</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="TPCA.fit_transform-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#TPCA.fit_transform"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="TPCA.fit_transform-193"><a href="#TPCA.fit_transform-193"><span class="linenos">193</span></a>    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span><span id="TPCA.fit_transform-194"><a href="#TPCA.fit_transform-194"><span class="linenos">194</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Fit the model with X and apply the dimensionality reduction on X.</span>
</span><span id="TPCA.fit_transform-195"><a href="#TPCA.fit_transform-195"><span class="linenos">195</span></a>
</span><span id="TPCA.fit_transform-196"><a href="#TPCA.fit_transform-196"><span class="linenos">196</span></a><span class="sd">        The output here will not be identical to calling fix(X).transform(X).</span>
</span><span id="TPCA.fit_transform-197"><a href="#TPCA.fit_transform-197"><span class="linenos">197</span></a><span class="sd">        But, the two give the same results up to machine precision. We</span>
</span><span id="TPCA.fit_transform-198"><a href="#TPCA.fit_transform-198"><span class="linenos">198</span></a><span class="sd">        provide a brief discussion of this below.</span>
</span><span id="TPCA.fit_transform-199"><a href="#TPCA.fit_transform-199"><span class="linenos">199</span></a>
</span><span id="TPCA.fit_transform-200"><a href="#TPCA.fit_transform-200"><span class="linenos">200</span></a><span class="sd">        Parameters</span>
</span><span id="TPCA.fit_transform-201"><a href="#TPCA.fit_transform-201"><span class="linenos">201</span></a><span class="sd">        ----------</span>
</span><span id="TPCA.fit_transform-202"><a href="#TPCA.fit_transform-202"><span class="linenos">202</span></a><span class="sd">        X : ndarray of shape (n, p, t)</span>
</span><span id="TPCA.fit_transform-203"><a href="#TPCA.fit_transform-203"><span class="linenos">203</span></a><span class="sd">            Training data, where `n` is the number of samples, `p` is the</span>
</span><span id="TPCA.fit_transform-204"><a href="#TPCA.fit_transform-204"><span class="linenos">204</span></a><span class="sd">            number of features, as `t` is the number of time points.</span>
</span><span id="TPCA.fit_transform-205"><a href="#TPCA.fit_transform-205"><span class="linenos">205</span></a>
</span><span id="TPCA.fit_transform-206"><a href="#TPCA.fit_transform-206"><span class="linenos">206</span></a><span class="sd">        y : Ignored</span>
</span><span id="TPCA.fit_transform-207"><a href="#TPCA.fit_transform-207"><span class="linenos">207</span></a><span class="sd">            Ignored.</span>
</span><span id="TPCA.fit_transform-208"><a href="#TPCA.fit_transform-208"><span class="linenos">208</span></a>
</span><span id="TPCA.fit_transform-209"><a href="#TPCA.fit_transform-209"><span class="linenos">209</span></a><span class="sd">        Returns</span>
</span><span id="TPCA.fit_transform-210"><a href="#TPCA.fit_transform-210"><span class="linenos">210</span></a><span class="sd">        -------</span>
</span><span id="TPCA.fit_transform-211"><a href="#TPCA.fit_transform-211"><span class="linenos">211</span></a><span class="sd">        X_transformed : ndarray of shape (n, n_components)</span>
</span><span id="TPCA.fit_transform-212"><a href="#TPCA.fit_transform-212"><span class="linenos">212</span></a><span class="sd">            TCAM projections in 2D transformed space.</span>
</span><span id="TPCA.fit_transform-213"><a href="#TPCA.fit_transform-213"><span class="linenos">213</span></a>
</span><span id="TPCA.fit_transform-214"><a href="#TPCA.fit_transform-214"><span class="linenos">214</span></a><span class="sd">        Notes</span>
</span><span id="TPCA.fit_transform-215"><a href="#TPCA.fit_transform-215"><span class="linenos">215</span></a><span class="sd">        -----</span>
</span><span id="TPCA.fit_transform-216"><a href="#TPCA.fit_transform-216"><span class="linenos">216</span></a><span class="sd">        The tensor m-product from Kilmer et al. (2021) has a notion of tensor</span>
</span><span id="TPCA.fit_transform-217"><a href="#TPCA.fit_transform-217"><span class="linenos">217</span></a><span class="sd">        inverse, and tensor orthogonality.</span>
</span><span id="TPCA.fit_transform-218"><a href="#TPCA.fit_transform-218"><span class="linenos">218</span></a>
</span><span id="TPCA.fit_transform-219"><a href="#TPCA.fit_transform-219"><span class="linenos">219</span></a><span class="sd">        We benchmarked an alternative approach as taken by sklearn in their</span>
</span><span id="TPCA.fit_transform-220"><a href="#TPCA.fit_transform-220"><span class="linenos">220</span></a><span class="sd">        PCA class. If we right multiply A&#39;s tSVDM by $V$ we note that it</span>
</span><span id="TPCA.fit_transform-221"><a href="#TPCA.fit_transform-221"><span class="linenos">221</span></a><span class="sd">        cancels the $V^T$ giving us:</span>
</span><span id="TPCA.fit_transform-222"><a href="#TPCA.fit_transform-222"><span class="linenos">222</span></a><span class="sd">            $$</span>
</span><span id="TPCA.fit_transform-223"><a href="#TPCA.fit_transform-223"><span class="linenos">223</span></a><span class="sd">                Z = A *_M V = U *_M S</span>
</span><span id="TPCA.fit_transform-224"><a href="#TPCA.fit_transform-224"><span class="linenos">224</span></a><span class="sd">            $$</span>
</span><span id="TPCA.fit_transform-225"><a href="#TPCA.fit_transform-225"><span class="linenos">225</span></a><span class="sd">        It appears that computing the final term is more computationally</span>
</span><span id="TPCA.fit_transform-226"><a href="#TPCA.fit_transform-226"><span class="linenos">226</span></a><span class="sd">        efficient, even if we have to convert $S$ into its full (sparse)</span>
</span><span id="TPCA.fit_transform-227"><a href="#TPCA.fit_transform-227"><span class="linenos">227</span></a><span class="sd">        tensor representation.</span>
</span><span id="TPCA.fit_transform-228"><a href="#TPCA.fit_transform-228"><span class="linenos">228</span></a>
</span><span id="TPCA.fit_transform-229"><a href="#TPCA.fit_transform-229"><span class="linenos">229</span></a><span class="sd">        By contrast, transform(X) will simply compute</span>
</span><span id="TPCA.fit_transform-230"><a href="#TPCA.fit_transform-230"><span class="linenos">230</span></a><span class="sd">            $$</span>
</span><span id="TPCA.fit_transform-231"><a href="#TPCA.fit_transform-231"><span class="linenos">231</span></a><span class="sd">                Z = A *_M V</span>
</span><span id="TPCA.fit_transform-232"><a href="#TPCA.fit_transform-232"><span class="linenos">232</span></a><span class="sd">            $$</span>
</span><span id="TPCA.fit_transform-233"><a href="#TPCA.fit_transform-233"><span class="linenos">233</span></a>
</span><span id="TPCA.fit_transform-234"><a href="#TPCA.fit_transform-234"><span class="linenos">234</span></a><span class="sd">        In both cases, $Z$ still needs to be converted by $\\times_3 M^{-1}$</span>
</span><span id="TPCA.fit_transform-235"><a href="#TPCA.fit_transform-235"><span class="linenos">235</span></a><span class="sd">        and &#39;compressed&#39; before being returned.</span>
</span><span id="TPCA.fit_transform-236"><a href="#TPCA.fit_transform-236"><span class="linenos">236</span></a><span class="sd">        For these details see Mor et al. (2022).</span>
</span><span id="TPCA.fit_transform-237"><a href="#TPCA.fit_transform-237"><span class="linenos">237</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="TPCA.fit_transform-238"><a href="#TPCA.fit_transform-238"><span class="linenos">238</span></a>        <span class="c1"># note that these tensors do NOT have full face-wise matrices</span>
</span><span id="TPCA.fit_transform-239"><a href="#TPCA.fit_transform-239"><span class="linenos">239</span></a>        <span class="n">hatU</span><span class="p">,</span> <span class="n">hatS_mat</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="TPCA.fit_transform-240"><a href="#TPCA.fit_transform-240"><span class="linenos">240</span></a>        <span class="n">hatS</span> <span class="o">=</span> <span class="n">_singular_vals_mat_to_tensor</span><span class="p">(</span><span class="n">hatS_mat</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span><span class="p">)</span>
</span><span id="TPCA.fit_transform-241"><a href="#TPCA.fit_transform-241"><span class="linenos">241</span></a>        <span class="k">return</span> <span class="n">facewise_product</span><span class="p">(</span><span class="n">hatU</span><span class="p">,</span> <span class="n">hatS</span><span class="p">)[</span>
</span><span id="TPCA.fit_transform-242"><a href="#TPCA.fit_transform-242"><span class="linenos">242</span></a>            <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_k_t_flatten_sort</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_k_t_flatten_sort</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="TPCA.fit_transform-243"><a href="#TPCA.fit_transform-243"><span class="linenos">243</span></a>        <span class="p">]</span>
</span></pre></div>


            <div class="docstring"><p>Fit the model with X and apply the dimensionality reduction on X.</p>

<p>The output here will not be identical to calling fix(X).transform(X).
But, the two give the same results up to machine precision. We
provide a brief discussion of this below.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>X</strong> (ndarray of shape (n, p, t)):
Training data, where <code>n</code> is the number of samples, <code>p</code> is the
number of features, as <code>t</code> is the number of time points.</li>
<li><strong>y</strong> (Ignored):
Ignored.</li>
</ul>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>X_transformed</strong> (ndarray of shape (n, n_components)):
TCAM projections in 2D transformed space.</li>
</ul>

<h6 id="notes">Notes</h6>

<p>The tensor m-product from Kilmer et al. (2021) has a notion of tensor
inverse, and tensor orthogonality.</p>

<p>We benchmarked an alternative approach as taken by sklearn in their
PCA class. If we right multiply A's tSVDM by $V$ we note that it
cancels the $V^T$ giving us:
    $$
        Z = A *_M V = U *_M S
    $$
It appears that computing the final term is more computationally
efficient, even if we have to convert $S$ into its full (sparse)
tensor representation.</p>

<p>By contrast, transform(X) will simply compute
    $$
        Z = A *_M V
    $$</p>

<p>In both cases, $Z$ still needs to be converted by $\times_3 M^{-1}$
and 'compressed' before being returned.
For these details see Mor et al. (2022).</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>sklearn.base.ClassNamePrefixFeaturesOutMixin</dt>
                                <dd id="TPCA.get_feature_names_out" class="function">get_feature_names_out</dd>

            </div>
            <div><dt>sklearn.utils._set_output._SetOutputMixin</dt>
                                <dd id="TPCA.set_output" class="function">set_output</dd>

            </div>
            <div><dt>sklearn.base.BaseEstimator</dt>
                                <dd id="TPCA.get_params" class="function">get_params</dd>
                <dd id="TPCA.set_params" class="function">set_params</dd>

            </div>
            <div><dt>sklearn.utils._metadata_requests._MetadataRequester</dt>
                                <dd id="TPCA.get_metadata_routing" class="function">get_metadata_routing</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="display_tensor_facewise">
                            <input id="display_tensor_facewise-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">display_tensor_facewise</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">tens</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="display_tensor_facewise-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#display_tensor_facewise"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="display_tensor_facewise-23"><a href="#display_tensor_facewise-23"><span class="linenos">23</span></a><span class="k">def</span> <span class="nf">display_tensor_facewise</span><span class="p">(</span><span class="n">tens</span><span class="p">):</span>
</span><span id="display_tensor_facewise-24"><a href="#display_tensor_facewise-24"><span class="linenos">24</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Display an order-3 tensor represented by a Numpy ndarray nicely.</span>
</span><span id="display_tensor_facewise-25"><a href="#display_tensor_facewise-25"><span class="linenos">25</span></a>
</span><span id="display_tensor_facewise-26"><a href="#display_tensor_facewise-26"><span class="linenos">26</span></a><span class="sd">    By default Numpy prints order-3 arrays as vertical stacks of order-2</span>
</span><span id="display_tensor_facewise-27"><a href="#display_tensor_facewise-27"><span class="linenos">27</span></a><span class="sd">    arrays in line with their broadcasting rules. This function prints a</span>
</span><span id="display_tensor_facewise-28"><a href="#display_tensor_facewise-28"><span class="linenos">28</span></a><span class="sd">    transpose view so the print output is a more intuitive sequence of frontal</span>
</span><span id="display_tensor_facewise-29"><a href="#display_tensor_facewise-29"><span class="linenos">29</span></a><span class="sd">    slices. It also prints the rounded values.</span>
</span><span id="display_tensor_facewise-30"><a href="#display_tensor_facewise-30"><span class="linenos">30</span></a>
</span><span id="display_tensor_facewise-31"><a href="#display_tensor_facewise-31"><span class="linenos">31</span></a><span class="sd">    We often use this in notebooks.</span>
</span><span id="display_tensor_facewise-32"><a href="#display_tensor_facewise-32"><span class="linenos">32</span></a>
</span><span id="display_tensor_facewise-33"><a href="#display_tensor_facewise-33"><span class="linenos">33</span></a><span class="sd">    UPDATE (v0.1.1): Also works for matrices and vectors now.</span>
</span><span id="display_tensor_facewise-34"><a href="#display_tensor_facewise-34"><span class="linenos">34</span></a>
</span><span id="display_tensor_facewise-35"><a href="#display_tensor_facewise-35"><span class="linenos">35</span></a><span class="sd">    Parameters</span>
</span><span id="display_tensor_facewise-36"><a href="#display_tensor_facewise-36"><span class="linenos">36</span></a><span class="sd">    ----------</span>
</span><span id="display_tensor_facewise-37"><a href="#display_tensor_facewise-37"><span class="linenos">37</span></a><span class="sd">    tens : ndarray of shape (n, p, t)</span>
</span><span id="display_tensor_facewise-38"><a href="#display_tensor_facewise-38"><span class="linenos">38</span></a><span class="sd">        Order-3 tensor.</span>
</span><span id="display_tensor_facewise-39"><a href="#display_tensor_facewise-39"><span class="linenos">39</span></a>
</span><span id="display_tensor_facewise-40"><a href="#display_tensor_facewise-40"><span class="linenos">40</span></a><span class="sd">    Examples</span>
</span><span id="display_tensor_facewise-41"><a href="#display_tensor_facewise-41"><span class="linenos">41</span></a><span class="sd">    --------</span>
</span><span id="display_tensor_facewise-42"><a href="#display_tensor_facewise-42"><span class="linenos">42</span></a><span class="sd">    &gt;&gt;&gt; import numpy as np</span>
</span><span id="display_tensor_facewise-43"><a href="#display_tensor_facewise-43"><span class="linenos">43</span></a><span class="sd">    &gt;&gt;&gt; from tdim import display_tensor_facewise as disp</span>
</span><span id="display_tensor_facewise-44"><a href="#display_tensor_facewise-44"><span class="linenos">44</span></a><span class="sd">    &gt;&gt;&gt; test = np.eye(3)[:, :, None] # a 3x3x1 tensor</span>
</span><span id="display_tensor_facewise-45"><a href="#display_tensor_facewise-45"><span class="linenos">45</span></a><span class="sd">    &gt;&gt;&gt; print(test) # Numpy ndarrays default __str__ method is not intuitive</span>
</span><span id="display_tensor_facewise-46"><a href="#display_tensor_facewise-46"><span class="linenos">46</span></a><span class="sd">    [[[1.]</span>
</span><span id="display_tensor_facewise-47"><a href="#display_tensor_facewise-47"><span class="linenos">47</span></a><span class="sd">    [0.]</span>
</span><span id="display_tensor_facewise-48"><a href="#display_tensor_facewise-48"><span class="linenos">48</span></a><span class="sd">    [0.]]</span>
</span><span id="display_tensor_facewise-49"><a href="#display_tensor_facewise-49"><span class="linenos">49</span></a><span class="sd">    [[0.]</span>
</span><span id="display_tensor_facewise-50"><a href="#display_tensor_facewise-50"><span class="linenos">50</span></a><span class="sd">    [1.]</span>
</span><span id="display_tensor_facewise-51"><a href="#display_tensor_facewise-51"><span class="linenos">51</span></a><span class="sd">    [0.]]</span>
</span><span id="display_tensor_facewise-52"><a href="#display_tensor_facewise-52"><span class="linenos">52</span></a><span class="sd">    [[0.]</span>
</span><span id="display_tensor_facewise-53"><a href="#display_tensor_facewise-53"><span class="linenos">53</span></a><span class="sd">    [0.]</span>
</span><span id="display_tensor_facewise-54"><a href="#display_tensor_facewise-54"><span class="linenos">54</span></a><span class="sd">    [1.]]]</span>
</span><span id="display_tensor_facewise-55"><a href="#display_tensor_facewise-55"><span class="linenos">55</span></a><span class="sd">    &gt;&gt;&gt; disp(test)</span>
</span><span id="display_tensor_facewise-56"><a href="#display_tensor_facewise-56"><span class="linenos">56</span></a><span class="sd">    Tensor with dimensions (3, 3, 1)</span>
</span><span id="display_tensor_facewise-57"><a href="#display_tensor_facewise-57"><span class="linenos">57</span></a><span class="sd">    [[[1. 0. 0.]</span>
</span><span id="display_tensor_facewise-58"><a href="#display_tensor_facewise-58"><span class="linenos">58</span></a><span class="sd">    [0. 1. 0.]</span>
</span><span id="display_tensor_facewise-59"><a href="#display_tensor_facewise-59"><span class="linenos">59</span></a><span class="sd">    [0. 0. 1.]]]</span>
</span><span id="display_tensor_facewise-60"><a href="#display_tensor_facewise-60"><span class="linenos">60</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="display_tensor_facewise-61"><a href="#display_tensor_facewise-61"><span class="linenos">61</span></a>    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tens</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="ow">in</span> <span class="p">(</span>
</span><span id="display_tensor_facewise-62"><a href="#display_tensor_facewise-62"><span class="linenos">62</span></a>        <span class="mi">1</span><span class="p">,</span>  <span class="c1"># vector</span>
</span><span id="display_tensor_facewise-63"><a href="#display_tensor_facewise-63"><span class="linenos">63</span></a>        <span class="mi">2</span><span class="p">,</span>  <span class="c1"># matrix</span>
</span><span id="display_tensor_facewise-64"><a href="#display_tensor_facewise-64"><span class="linenos">64</span></a>        <span class="mi">3</span><span class="p">,</span>  <span class="c1"># tensor</span>
</span><span id="display_tensor_facewise-65"><a href="#display_tensor_facewise-65"><span class="linenos">65</span></a>    <span class="p">),</span> <span class="s2">&quot;Expecting 1D (vector), 2D (matrix) or 3D (tensor) input&quot;</span>
</span><span id="display_tensor_facewise-66"><a href="#display_tensor_facewise-66"><span class="linenos">66</span></a>
</span><span id="display_tensor_facewise-67"><a href="#display_tensor_facewise-67"><span class="linenos">67</span></a>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tens</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
</span><span id="display_tensor_facewise-68"><a href="#display_tensor_facewise-68"><span class="linenos">68</span></a>        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tensor with shape </span><span class="si">{</span><span class="n">tens</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="display_tensor_facewise-69"><a href="#display_tensor_facewise-69"><span class="linenos">69</span></a>        <span class="nb">print</span><span class="p">(</span><span class="n">tens</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">6</span><span class="p">))</span>
</span><span id="display_tensor_facewise-70"><a href="#display_tensor_facewise-70"><span class="linenos">70</span></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="display_tensor_facewise-71"><a href="#display_tensor_facewise-71"><span class="linenos">71</span></a>        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tens</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="display_tensor_facewise-72"><a href="#display_tensor_facewise-72"><span class="linenos">72</span></a>            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Matrix with shape </span><span class="si">{</span><span class="n">tens</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="display_tensor_facewise-73"><a href="#display_tensor_facewise-73"><span class="linenos">73</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="display_tensor_facewise-74"><a href="#display_tensor_facewise-74"><span class="linenos">74</span></a>            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vector with length </span><span class="si">{</span><span class="n">tens</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="display_tensor_facewise-75"><a href="#display_tensor_facewise-75"><span class="linenos">75</span></a>        <span class="nb">print</span><span class="p">(</span><span class="n">tens</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">6</span><span class="p">))</span>
</span></pre></div>


            <div class="docstring"><p>Display an order-3 tensor represented by a Numpy ndarray nicely.</p>

<p>By default Numpy prints order-3 arrays as vertical stacks of order-2
arrays in line with their broadcasting rules. This function prints a
transpose view so the print output is a more intuitive sequence of frontal
slices. It also prints the rounded values.</p>

<p>We often use this in notebooks.</p>

<p>UPDATE (v0.1.1): Also works for matrices and vectors now.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>tens</strong> (ndarray of shape (n, p, t)):
Order-3 tensor.</li>
</ul>

<h6 id="examples">Examples</h6>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">tdim</span> <span class="kn">import</span> <span class="n">display_tensor_facewise</span> <span class="k">as</span> <span class="n">disp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="c1"># a 3x3x1 tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">test</span><span class="p">)</span> <span class="c1"># Numpy ndarrays default __str__ method is not intuitive</span>
<span class="go">[[[1.]</span>
<span class="go">[0.]</span>
<span class="go">[0.]]</span>
<span class="go">[[0.]</span>
<span class="go">[1.]</span>
<span class="go">[0.]]</span>
<span class="go">[[0.]</span>
<span class="go">[0.]</span>
<span class="go">[1.]]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">disp</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
<span class="go">Tensor with dimensions (3, 3, 1)</span>
<span class="go">[[[1. 0. 0.]</span>
<span class="go">[0. 1. 0.]</span>
<span class="go">[0. 0. 1.]]]</span>
</code></pre>
</div>
</div>


                </section>
                <section id="generate_default_m_transform_pair">
                            <input id="generate_default_m_transform_pair-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">generate_default_m_transform_pair</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">t</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="generate_default_m_transform_pair-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#generate_default_m_transform_pair"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="generate_default_m_transform_pair-29"><a href="#generate_default_m_transform_pair-29"><span class="linenos">29</span></a><span class="k">def</span> <span class="nf">generate_default_m_transform_pair</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
</span><span id="generate_default_m_transform_pair-30"><a href="#generate_default_m_transform_pair-30"><span class="linenos">30</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Return the default `M, Minv` used by `tred` algorithms.</span>
</span><span id="generate_default_m_transform_pair-31"><a href="#generate_default_m_transform_pair-31"><span class="linenos">31</span></a>
</span><span id="generate_default_m_transform_pair-32"><a href="#generate_default_m_transform_pair-32"><span class="linenos">32</span></a><span class="sd">    We do not guarantee that the default m-transform used in `tred` will remain</span>
</span><span id="generate_default_m_transform_pair-33"><a href="#generate_default_m_transform_pair-33"><span class="linenos">33</span></a><span class="sd">    consistent between versions, as this may change depending on recent</span>
</span><span id="generate_default_m_transform_pair-34"><a href="#generate_default_m_transform_pair-34"><span class="linenos">34</span></a><span class="sd">    research and literature. We highly encourage `tred` users to explicitly</span>
</span><span id="generate_default_m_transform_pair-35"><a href="#generate_default_m_transform_pair-35"><span class="linenos">35</span></a><span class="sd">    define M and Minv in the calling state to &#39;future-proof&#39; your code.</span>
</span><span id="generate_default_m_transform_pair-36"><a href="#generate_default_m_transform_pair-36"><span class="linenos">36</span></a>
</span><span id="generate_default_m_transform_pair-37"><a href="#generate_default_m_transform_pair-37"><span class="linenos">37</span></a><span class="sd">    Parameters</span>
</span><span id="generate_default_m_transform_pair-38"><a href="#generate_default_m_transform_pair-38"><span class="linenos">38</span></a><span class="sd">    ----------</span>
</span><span id="generate_default_m_transform_pair-39"><a href="#generate_default_m_transform_pair-39"><span class="linenos">39</span></a><span class="sd">    t : int</span>
</span><span id="generate_default_m_transform_pair-40"><a href="#generate_default_m_transform_pair-40"><span class="linenos">40</span></a><span class="sd">        The length of the tubal fibers of the target tensors, i.e. the size of</span>
</span><span id="generate_default_m_transform_pair-41"><a href="#generate_default_m_transform_pair-41"><span class="linenos">41</span></a><span class="sd">        its third dimension.</span>
</span><span id="generate_default_m_transform_pair-42"><a href="#generate_default_m_transform_pair-42"><span class="linenos">42</span></a>
</span><span id="generate_default_m_transform_pair-43"><a href="#generate_default_m_transform_pair-43"><span class="linenos">43</span></a><span class="sd">    Returns</span>
</span><span id="generate_default_m_transform_pair-44"><a href="#generate_default_m_transform_pair-44"><span class="linenos">44</span></a><span class="sd">    -------</span>
</span><span id="generate_default_m_transform_pair-45"><a href="#generate_default_m_transform_pair-45"><span class="linenos">45</span></a><span class="sd">    M : Callable[[ndarray], ndarray]</span>
</span><span id="generate_default_m_transform_pair-46"><a href="#generate_default_m_transform_pair-46"><span class="linenos">46</span></a><span class="sd">        A function which expects an order-3 tensor as input, and returns the</span>
</span><span id="generate_default_m_transform_pair-47"><a href="#generate_default_m_transform_pair-47"><span class="linenos">47</span></a><span class="sd">        image under the default m-transform being used in `tred`</span>
</span><span id="generate_default_m_transform_pair-48"><a href="#generate_default_m_transform_pair-48"><span class="linenos">48</span></a>
</span><span id="generate_default_m_transform_pair-49"><a href="#generate_default_m_transform_pair-49"><span class="linenos">49</span></a><span class="sd">    Minv : Callable[[ndarray], ndarray]</span>
</span><span id="generate_default_m_transform_pair-50"><a href="#generate_default_m_transform_pair-50"><span class="linenos">50</span></a><span class="sd">        A function implementing the inverse transform of `M`.</span>
</span><span id="generate_default_m_transform_pair-51"><a href="#generate_default_m_transform_pair-51"><span class="linenos">51</span></a>
</span><span id="generate_default_m_transform_pair-52"><a href="#generate_default_m_transform_pair-52"><span class="linenos">52</span></a><span class="sd">    References</span>
</span><span id="generate_default_m_transform_pair-53"><a href="#generate_default_m_transform_pair-53"><span class="linenos">53</span></a><span class="sd">    ----------</span>
</span><span id="generate_default_m_transform_pair-54"><a href="#generate_default_m_transform_pair-54"><span class="linenos">54</span></a><span class="sd">    Kilmer, M.E., Horesh, L., Avron, H. and Newman, E., 2021. Tensor-tensor</span>
</span><span id="generate_default_m_transform_pair-55"><a href="#generate_default_m_transform_pair-55"><span class="linenos">55</span></a><span class="sd">    algebra for optimal representation and compression of multiway data.</span>
</span><span id="generate_default_m_transform_pair-56"><a href="#generate_default_m_transform_pair-56"><span class="linenos">56</span></a><span class="sd">    Proceedings of the National Academy of Sciences, 118(28), p.e2015851118.</span>
</span><span id="generate_default_m_transform_pair-57"><a href="#generate_default_m_transform_pair-57"><span class="linenos">57</span></a>
</span><span id="generate_default_m_transform_pair-58"><a href="#generate_default_m_transform_pair-58"><span class="linenos">58</span></a><span class="sd">    Mor, U., Cohen, Y., Valdés-Mas, R., Kviatcovsky, D., Elinav, E. and Avron,</span>
</span><span id="generate_default_m_transform_pair-59"><a href="#generate_default_m_transform_pair-59"><span class="linenos">59</span></a><span class="sd">    H., 2022. Dimensionality reduction of longitudinal’omics data using modern</span>
</span><span id="generate_default_m_transform_pair-60"><a href="#generate_default_m_transform_pair-60"><span class="linenos">60</span></a><span class="sd">    tensor factorizations. PLoS Computational Biology, 18(7), p.e1010212.</span>
</span><span id="generate_default_m_transform_pair-61"><a href="#generate_default_m_transform_pair-61"><span class="linenos">61</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="generate_default_m_transform_pair-62"><a href="#generate_default_m_transform_pair-62"><span class="linenos">62</span></a>    <span class="k">return</span> <span class="n">generate_dctii_m_transform_pair</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Return the default <code>M, Minv</code> used by <code><a href="">tred</a></code> algorithms.</p>

<p>We do not guarantee that the default m-transform used in <code><a href="">tred</a></code> will remain
consistent between versions, as this may change depending on recent
research and literature. We highly encourage <code><a href="">tred</a></code> users to explicitly
define M and Minv in the calling state to 'future-proof' your code.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>t</strong> (int):
The length of the tubal fibers of the target tensors, i.e. the size of
its third dimension.</li>
</ul>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>M</strong> (Callable[[ndarray], ndarray]):
A function which expects an order-3 tensor as input, and returns the
image under the default m-transform being used in <code><a href="">tred</a></code></li>
<li><strong>Minv</strong> (Callable[[ndarray], ndarray]):
A function implementing the inverse transform of <code>M</code>.</li>
</ul>

<h6 id="references">References</h6>

<p>Kilmer, M.E., Horesh, L., Avron, H. and Newman, E., 2021. Tensor-tensor
algebra for optimal representation and compression of multiway data.
Proceedings of the National Academy of Sciences, 118(28), p.e2015851118.</p>

<p>Mor, U., Cohen, Y., Valdés-Mas, R., Kviatcovsky, D., Elinav, E. and Avron,
H., 2022. Dimensionality reduction of longitudinal’omics data using modern
tensor factorizations. PLoS Computational Biology, 18(7), p.e1010212.</p>
</div>


                </section>
                <section id="generate_transform_pair_from_matrix">
                            <input id="generate_transform_pair_from_matrix-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">generate_transform_pair_from_matrix</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">M_mat</span>, </span><span class="param"><span class="n">Minv_mat</span><span class="o">=</span><span class="kc">None</span>, </span><span class="param"><span class="o">*</span>, </span><span class="param"><span class="n">inplace</span><span class="o">=</span><span class="kc">False</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="generate_transform_pair_from_matrix-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#generate_transform_pair_from_matrix"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="generate_transform_pair_from_matrix-65"><a href="#generate_transform_pair_from_matrix-65"><span class="linenos"> 65</span></a><span class="k">def</span> <span class="nf">generate_transform_pair_from_matrix</span><span class="p">(</span><span class="n">M_mat</span><span class="p">,</span> <span class="n">Minv_mat</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span><span id="generate_transform_pair_from_matrix-66"><a href="#generate_transform_pair_from_matrix-66"><span class="linenos"> 66</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Return `M, Minv` as defined by an orthogonal matrix.</span>
</span><span id="generate_transform_pair_from_matrix-67"><a href="#generate_transform_pair_from_matrix-67"><span class="linenos"> 67</span></a>
</span><span id="generate_transform_pair_from_matrix-68"><a href="#generate_transform_pair_from_matrix-68"><span class="linenos"> 68</span></a><span class="sd">    Allows the user to specify any orthogonal matrix, and this function will</span>
</span><span id="generate_transform_pair_from_matrix-69"><a href="#generate_transform_pair_from_matrix-69"><span class="linenos"> 69</span></a><span class="sd">    infer `t`, and numerically compute the inverse, returning functions `M`</span>
</span><span id="generate_transform_pair_from_matrix-70"><a href="#generate_transform_pair_from_matrix-70"><span class="linenos"> 70</span></a><span class="sd">    and `Minv` which can be used for `tred` algorithms.</span>
</span><span id="generate_transform_pair_from_matrix-71"><a href="#generate_transform_pair_from_matrix-71"><span class="linenos"> 71</span></a>
</span><span id="generate_transform_pair_from_matrix-72"><a href="#generate_transform_pair_from_matrix-72"><span class="linenos"> 72</span></a><span class="sd">    Optionally, the user can also choose to specify the inverse explicitly.</span>
</span><span id="generate_transform_pair_from_matrix-73"><a href="#generate_transform_pair_from_matrix-73"><span class="linenos"> 73</span></a>
</span><span id="generate_transform_pair_from_matrix-74"><a href="#generate_transform_pair_from_matrix-74"><span class="linenos"> 74</span></a><span class="sd">    Parameters</span>
</span><span id="generate_transform_pair_from_matrix-75"><a href="#generate_transform_pair_from_matrix-75"><span class="linenos"> 75</span></a><span class="sd">    ----------</span>
</span><span id="generate_transform_pair_from_matrix-76"><a href="#generate_transform_pair_from_matrix-76"><span class="linenos"> 76</span></a><span class="sd">    M_mat : ndarray</span>
</span><span id="generate_transform_pair_from_matrix-77"><a href="#generate_transform_pair_from_matrix-77"><span class="linenos"> 77</span></a><span class="sd">        Orthogonal square matrix.</span>
</span><span id="generate_transform_pair_from_matrix-78"><a href="#generate_transform_pair_from_matrix-78"><span class="linenos"> 78</span></a>
</span><span id="generate_transform_pair_from_matrix-79"><a href="#generate_transform_pair_from_matrix-79"><span class="linenos"> 79</span></a><span class="sd">    Minv_mat : ndarray or None, default=None</span>
</span><span id="generate_transform_pair_from_matrix-80"><a href="#generate_transform_pair_from_matrix-80"><span class="linenos"> 80</span></a><span class="sd">        Square matrix, the inverse of M_mat. If not specified, this function</span>
</span><span id="generate_transform_pair_from_matrix-81"><a href="#generate_transform_pair_from_matrix-81"><span class="linenos"> 81</span></a><span class="sd">        will numerically evaluate the inverse of `M_mat`.</span>
</span><span id="generate_transform_pair_from_matrix-82"><a href="#generate_transform_pair_from_matrix-82"><span class="linenos"> 82</span></a>
</span><span id="generate_transform_pair_from_matrix-83"><a href="#generate_transform_pair_from_matrix-83"><span class="linenos"> 83</span></a><span class="sd">    inplace : bool, default=False</span>
</span><span id="generate_transform_pair_from_matrix-84"><a href="#generate_transform_pair_from_matrix-84"><span class="linenos"> 84</span></a><span class="sd">        *Placeholder for future development*</span>
</span><span id="generate_transform_pair_from_matrix-85"><a href="#generate_transform_pair_from_matrix-85"><span class="linenos"> 85</span></a>
</span><span id="generate_transform_pair_from_matrix-86"><a href="#generate_transform_pair_from_matrix-86"><span class="linenos"> 86</span></a><span class="sd">    Returns</span>
</span><span id="generate_transform_pair_from_matrix-87"><a href="#generate_transform_pair_from_matrix-87"><span class="linenos"> 87</span></a><span class="sd">    -------</span>
</span><span id="generate_transform_pair_from_matrix-88"><a href="#generate_transform_pair_from_matrix-88"><span class="linenos"> 88</span></a><span class="sd">    M : Callable[[ndarray], ndarray]</span>
</span><span id="generate_transform_pair_from_matrix-89"><a href="#generate_transform_pair_from_matrix-89"><span class="linenos"> 89</span></a><span class="sd">        A function which expects an order-3 tensor as input, and applies</span>
</span><span id="generate_transform_pair_from_matrix-90"><a href="#generate_transform_pair_from_matrix-90"><span class="linenos"> 90</span></a><span class="sd">        `M_mat` to each of the tubal fibres. This preserves the dimensions of</span>
</span><span id="generate_transform_pair_from_matrix-91"><a href="#generate_transform_pair_from_matrix-91"><span class="linenos"> 91</span></a><span class="sd">        the tensor.</span>
</span><span id="generate_transform_pair_from_matrix-92"><a href="#generate_transform_pair_from_matrix-92"><span class="linenos"> 92</span></a>
</span><span id="generate_transform_pair_from_matrix-93"><a href="#generate_transform_pair_from_matrix-93"><span class="linenos"> 93</span></a><span class="sd">    Minv : Callable[[ndarray], ndarray]</span>
</span><span id="generate_transform_pair_from_matrix-94"><a href="#generate_transform_pair_from_matrix-94"><span class="linenos"> 94</span></a><span class="sd">        A function implementing the inverse transform of `M`.</span>
</span><span id="generate_transform_pair_from_matrix-95"><a href="#generate_transform_pair_from_matrix-95"><span class="linenos"> 95</span></a>
</span><span id="generate_transform_pair_from_matrix-96"><a href="#generate_transform_pair_from_matrix-96"><span class="linenos"> 96</span></a><span class="sd">    References</span>
</span><span id="generate_transform_pair_from_matrix-97"><a href="#generate_transform_pair_from_matrix-97"><span class="linenos"> 97</span></a><span class="sd">    ----------</span>
</span><span id="generate_transform_pair_from_matrix-98"><a href="#generate_transform_pair_from_matrix-98"><span class="linenos"> 98</span></a><span class="sd">    Kilmer, M.E., Horesh, L., Avron, H. and Newman, E., 2021. Tensor-tensor</span>
</span><span id="generate_transform_pair_from_matrix-99"><a href="#generate_transform_pair_from_matrix-99"><span class="linenos"> 99</span></a><span class="sd">    algebra for optimal representation and compression of multiway data.</span>
</span><span id="generate_transform_pair_from_matrix-100"><a href="#generate_transform_pair_from_matrix-100"><span class="linenos">100</span></a><span class="sd">    Proceedings of the National Academy of Sciences, 118(28), p.e2015851118.</span>
</span><span id="generate_transform_pair_from_matrix-101"><a href="#generate_transform_pair_from_matrix-101"><span class="linenos">101</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="generate_transform_pair_from_matrix-102"><a href="#generate_transform_pair_from_matrix-102"><span class="linenos">102</span></a>    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">M_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;Expecting matrix (order-2 array) input&quot;</span>
</span><span id="generate_transform_pair_from_matrix-103"><a href="#generate_transform_pair_from_matrix-103"><span class="linenos">103</span></a>    <span class="k">assert</span> <span class="n">M_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">M_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;Expecting square matrix input&quot;</span>
</span><span id="generate_transform_pair_from_matrix-104"><a href="#generate_transform_pair_from_matrix-104"><span class="linenos">104</span></a>    <span class="n">t_</span> <span class="o">=</span> <span class="n">M_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="generate_transform_pair_from_matrix-105"><a href="#generate_transform_pair_from_matrix-105"><span class="linenos">105</span></a>
</span><span id="generate_transform_pair_from_matrix-106"><a href="#generate_transform_pair_from_matrix-106"><span class="linenos">106</span></a>    <span class="c1"># numerically evaluate inverse matrix if not explicitly specified</span>
</span><span id="generate_transform_pair_from_matrix-107"><a href="#generate_transform_pair_from_matrix-107"><span class="linenos">107</span></a>    <span class="k">if</span> <span class="n">Minv_mat</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="generate_transform_pair_from_matrix-108"><a href="#generate_transform_pair_from_matrix-108"><span class="linenos">108</span></a>        <span class="c1"># be pedantic and check for invertibility</span>
</span><span id="generate_transform_pair_from_matrix-109"><a href="#generate_transform_pair_from_matrix-109"><span class="linenos">109</span></a>        <span class="c1"># https://stackoverflow.com/questions/13249108/efficient-pythonic-check-for-singular-matrix</span>
</span><span id="generate_transform_pair_from_matrix-110"><a href="#generate_transform_pair_from_matrix-110"><span class="linenos">110</span></a>        <span class="c1"># NOTE: revisit later, is the following more robust?:</span>
</span><span id="generate_transform_pair_from_matrix-111"><a href="#generate_transform_pair_from_matrix-111"><span class="linenos">111</span></a>        <span class="c1"># https://stackoverflow.com/questions/17931613/how-to-decide-a-whether-a-matrix-is-singular-in-python-numpy</span>
</span><span id="generate_transform_pair_from_matrix-112"><a href="#generate_transform_pair_from_matrix-112"><span class="linenos">112</span></a>        <span class="k">if</span> <span class="n">linalg</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">M_mat</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">sys</span><span class="o">.</span><span class="n">float_info</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
</span><span id="generate_transform_pair_from_matrix-113"><a href="#generate_transform_pair_from_matrix-113"><span class="linenos">113</span></a>            <span class="n">Minv_mat</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">M_mat</span><span class="p">)</span>
</span><span id="generate_transform_pair_from_matrix-114"><a href="#generate_transform_pair_from_matrix-114"><span class="linenos">114</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="generate_transform_pair_from_matrix-115"><a href="#generate_transform_pair_from_matrix-115"><span class="linenos">115</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="generate_transform_pair_from_matrix-116"><a href="#generate_transform_pair_from_matrix-116"><span class="linenos">116</span></a>                <span class="s2">&quot;Input matrix must be invertible, but appears singular, or close to singular&quot;</span>
</span><span id="generate_transform_pair_from_matrix-117"><a href="#generate_transform_pair_from_matrix-117"><span class="linenos">117</span></a>            <span class="p">)</span>
</span><span id="generate_transform_pair_from_matrix-118"><a href="#generate_transform_pair_from_matrix-118"><span class="linenos">118</span></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="generate_transform_pair_from_matrix-119"><a href="#generate_transform_pair_from_matrix-119"><span class="linenos">119</span></a>        <span class="k">assert</span> <span class="p">(</span>
</span><span id="generate_transform_pair_from_matrix-120"><a href="#generate_transform_pair_from_matrix-120"><span class="linenos">120</span></a>            <span class="n">Minv_mat</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">M_mat</span><span class="o">.</span><span class="n">shape</span>
</span><span id="generate_transform_pair_from_matrix-121"><a href="#generate_transform_pair_from_matrix-121"><span class="linenos">121</span></a>        <span class="p">),</span> <span class="s2">&quot;Ensure the shapes of matrix M and its inverse are the same&quot;</span>
</span><span id="generate_transform_pair_from_matrix-122"><a href="#generate_transform_pair_from_matrix-122"><span class="linenos">122</span></a>
</span><span id="generate_transform_pair_from_matrix-123"><a href="#generate_transform_pair_from_matrix-123"><span class="linenos">123</span></a>    <span class="c1"># a quick way of applying M along the tubal fibres of X, using numpy</span>
</span><span id="generate_transform_pair_from_matrix-124"><a href="#generate_transform_pair_from_matrix-124"><span class="linenos">124</span></a>    <span class="c1"># broadcasting: transpose X into a (n x t x p) vertical stack of (t x p)</span>
</span><span id="generate_transform_pair_from_matrix-125"><a href="#generate_transform_pair_from_matrix-125"><span class="linenos">125</span></a>    <span class="c1"># matrices. matrix multiplication is broadcast vertically, whereby M left</span>
</span><span id="generate_transform_pair_from_matrix-126"><a href="#generate_transform_pair_from_matrix-126"><span class="linenos">126</span></a>    <span class="c1"># multiplies each of the t x p matrices, effectively applying the</span>
</span><span id="generate_transform_pair_from_matrix-127"><a href="#generate_transform_pair_from_matrix-127"><span class="linenos">127</span></a>    <span class="c1"># transform to the columns, which were the original tubal fibres.</span>
</span><span id="generate_transform_pair_from_matrix-128"><a href="#generate_transform_pair_from_matrix-128"><span class="linenos">128</span></a>    <span class="c1"># then just perform the reverse transposition to get the original</span>
</span><span id="generate_transform_pair_from_matrix-129"><a href="#generate_transform_pair_from_matrix-129"><span class="linenos">129</span></a>    <span class="c1"># orientation of the tensor</span>
</span><span id="generate_transform_pair_from_matrix-130"><a href="#generate_transform_pair_from_matrix-130"><span class="linenos">130</span></a>    <span class="k">def</span> <span class="nf">M</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
</span><span id="generate_transform_pair_from_matrix-131"><a href="#generate_transform_pair_from_matrix-131"><span class="linenos">131</span></a>        <span class="n">_assert_t_and_order</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">t_</span><span class="p">)</span>
</span><span id="generate_transform_pair_from_matrix-132"><a href="#generate_transform_pair_from_matrix-132"><span class="linenos">132</span></a>        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
</span><span id="generate_transform_pair_from_matrix-133"><a href="#generate_transform_pair_from_matrix-133"><span class="linenos">133</span></a>            <span class="k">return</span> <span class="p">(</span><span class="n">M_mat</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="generate_transform_pair_from_matrix-134"><a href="#generate_transform_pair_from_matrix-134"><span class="linenos">134</span></a>        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="generate_transform_pair_from_matrix-135"><a href="#generate_transform_pair_from_matrix-135"><span class="linenos">135</span></a>            <span class="k">return</span> <span class="p">(</span><span class="n">M_mat</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</span><span id="generate_transform_pair_from_matrix-136"><a href="#generate_transform_pair_from_matrix-136"><span class="linenos">136</span></a>        <span class="k">else</span><span class="p">:</span>  <span class="c1"># len(X.shape == 1)</span>
</span><span id="generate_transform_pair_from_matrix-137"><a href="#generate_transform_pair_from_matrix-137"><span class="linenos">137</span></a>            <span class="k">return</span> <span class="n">M_mat</span> <span class="o">@</span> <span class="n">X</span>
</span><span id="generate_transform_pair_from_matrix-138"><a href="#generate_transform_pair_from_matrix-138"><span class="linenos">138</span></a>
</span><span id="generate_transform_pair_from_matrix-139"><a href="#generate_transform_pair_from_matrix-139"><span class="linenos">139</span></a>    <span class="k">def</span> <span class="nf">Minv</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
</span><span id="generate_transform_pair_from_matrix-140"><a href="#generate_transform_pair_from_matrix-140"><span class="linenos">140</span></a>        <span class="n">_assert_t_and_order</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">t_</span><span class="p">)</span>
</span><span id="generate_transform_pair_from_matrix-141"><a href="#generate_transform_pair_from_matrix-141"><span class="linenos">141</span></a>        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
</span><span id="generate_transform_pair_from_matrix-142"><a href="#generate_transform_pair_from_matrix-142"><span class="linenos">142</span></a>            <span class="k">return</span> <span class="p">(</span><span class="n">Minv_mat</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="generate_transform_pair_from_matrix-143"><a href="#generate_transform_pair_from_matrix-143"><span class="linenos">143</span></a>        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="generate_transform_pair_from_matrix-144"><a href="#generate_transform_pair_from_matrix-144"><span class="linenos">144</span></a>            <span class="k">return</span> <span class="p">(</span><span class="n">Minv_mat</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</span><span id="generate_transform_pair_from_matrix-145"><a href="#generate_transform_pair_from_matrix-145"><span class="linenos">145</span></a>        <span class="k">else</span><span class="p">:</span>  <span class="c1"># len(X.shape == 1)</span>
</span><span id="generate_transform_pair_from_matrix-146"><a href="#generate_transform_pair_from_matrix-146"><span class="linenos">146</span></a>            <span class="k">return</span> <span class="n">Minv_mat</span> <span class="o">@</span> <span class="n">X</span>
</span><span id="generate_transform_pair_from_matrix-147"><a href="#generate_transform_pair_from_matrix-147"><span class="linenos">147</span></a>
</span><span id="generate_transform_pair_from_matrix-148"><a href="#generate_transform_pair_from_matrix-148"><span class="linenos">148</span></a>    <span class="k">return</span> <span class="n">M</span><span class="p">,</span> <span class="n">Minv</span>
</span></pre></div>


            <div class="docstring"><p>Return <code>M, Minv</code> as defined by an orthogonal matrix.</p>

<p>Allows the user to specify any orthogonal matrix, and this function will
infer <code>t</code>, and numerically compute the inverse, returning functions <code>M</code>
and <code>Minv</code> which can be used for <code><a href="">tred</a></code> algorithms.</p>

<p>Optionally, the user can also choose to specify the inverse explicitly.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>M_mat</strong> (ndarray):
Orthogonal square matrix.</li>
<li><strong>Minv_mat</strong> (ndarray or None, default=None):
Square matrix, the inverse of M_mat. If not specified, this function
will numerically evaluate the inverse of <code>M_mat</code>.</li>
<li><strong>inplace</strong> (bool, default=False):
<em>Placeholder for future development</em></li>
</ul>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>M</strong> (Callable[[ndarray], ndarray]):
A function which expects an order-3 tensor as input, and applies
<code>M_mat</code> to each of the tubal fibres. This preserves the dimensions of
the tensor.</li>
<li><strong>Minv</strong> (Callable[[ndarray], ndarray]):
A function implementing the inverse transform of <code>M</code>.</li>
</ul>

<h6 id="references">References</h6>

<p>Kilmer, M.E., Horesh, L., Avron, H. and Newman, E., 2021. Tensor-tensor
algebra for optimal representation and compression of multiway data.
Proceedings of the National Academy of Sciences, 118(28), p.e2015851118.</p>
</div>


                </section>
                <section id="generate_dctii_m_transform_pair">
                            <input id="generate_dctii_m_transform_pair-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">generate_dctii_m_transform_pair</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">t</span>, </span><span class="param"><span class="o">*</span>, </span><span class="param"><span class="n">inplace</span><span class="o">=</span><span class="kc">False</span>, </span><span class="param"><span class="n">norm</span><span class="o">=</span><span class="s1">&#39;ortho&#39;</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="generate_dctii_m_transform_pair-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#generate_dctii_m_transform_pair"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="generate_dctii_m_transform_pair-151"><a href="#generate_dctii_m_transform_pair-151"><span class="linenos">151</span></a><span class="k">def</span> <span class="nf">generate_dctii_m_transform_pair</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s2">&quot;ortho&quot;</span><span class="p">):</span>
</span><span id="generate_dctii_m_transform_pair-152"><a href="#generate_dctii_m_transform_pair-152"><span class="linenos">152</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Return `M, Minv` as defined by the Discrete Cosine Transform of</span>
</span><span id="generate_dctii_m_transform_pair-153"><a href="#generate_dctii_m_transform_pair-153"><span class="linenos">153</span></a><span class="sd">    length `t`.</span>
</span><span id="generate_dctii_m_transform_pair-154"><a href="#generate_dctii_m_transform_pair-154"><span class="linenos">154</span></a>
</span><span id="generate_dctii_m_transform_pair-155"><a href="#generate_dctii_m_transform_pair-155"><span class="linenos">155</span></a><span class="sd">    Bascially a wrapper around scipy.fft to generate functions to perform</span>
</span><span id="generate_dctii_m_transform_pair-156"><a href="#generate_dctii_m_transform_pair-156"><span class="linenos">156</span></a><span class="sd">    fourier transform based $\\times_3 M$ operations, as used by</span>
</span><span id="generate_dctii_m_transform_pair-157"><a href="#generate_dctii_m_transform_pair-157"><span class="linenos">157</span></a><span class="sd">    Mor et al. (2022)</span>
</span><span id="generate_dctii_m_transform_pair-158"><a href="#generate_dctii_m_transform_pair-158"><span class="linenos">158</span></a>
</span><span id="generate_dctii_m_transform_pair-159"><a href="#generate_dctii_m_transform_pair-159"><span class="linenos">159</span></a><span class="sd">    Parameters</span>
</span><span id="generate_dctii_m_transform_pair-160"><a href="#generate_dctii_m_transform_pair-160"><span class="linenos">160</span></a><span class="sd">    ----------</span>
</span><span id="generate_dctii_m_transform_pair-161"><a href="#generate_dctii_m_transform_pair-161"><span class="linenos">161</span></a><span class="sd">    t : int</span>
</span><span id="generate_dctii_m_transform_pair-162"><a href="#generate_dctii_m_transform_pair-162"><span class="linenos">162</span></a><span class="sd">        The length of the transform</span>
</span><span id="generate_dctii_m_transform_pair-163"><a href="#generate_dctii_m_transform_pair-163"><span class="linenos">163</span></a>
</span><span id="generate_dctii_m_transform_pair-164"><a href="#generate_dctii_m_transform_pair-164"><span class="linenos">164</span></a><span class="sd">    inplace : bool, default=False</span>
</span><span id="generate_dctii_m_transform_pair-165"><a href="#generate_dctii_m_transform_pair-165"><span class="linenos">165</span></a><span class="sd">        Control whether or not the generated functions modify the input tensor</span>
</span><span id="generate_dctii_m_transform_pair-166"><a href="#generate_dctii_m_transform_pair-166"><span class="linenos">166</span></a><span class="sd">        in-place, or return a copy with the m-transform applied</span>
</span><span id="generate_dctii_m_transform_pair-167"><a href="#generate_dctii_m_transform_pair-167"><span class="linenos">167</span></a>
</span><span id="generate_dctii_m_transform_pair-168"><a href="#generate_dctii_m_transform_pair-168"><span class="linenos">168</span></a><span class="sd">    norm : {“backward”, “ortho”, “forward”}, default=&quot;ortho&quot;</span>
</span><span id="generate_dctii_m_transform_pair-169"><a href="#generate_dctii_m_transform_pair-169"><span class="linenos">169</span></a><span class="sd">        See https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dct.html#scipy.fft.dct</span>
</span><span id="generate_dctii_m_transform_pair-170"><a href="#generate_dctii_m_transform_pair-170"><span class="linenos">170</span></a>
</span><span id="generate_dctii_m_transform_pair-171"><a href="#generate_dctii_m_transform_pair-171"><span class="linenos">171</span></a><span class="sd">    Returns</span>
</span><span id="generate_dctii_m_transform_pair-172"><a href="#generate_dctii_m_transform_pair-172"><span class="linenos">172</span></a><span class="sd">    -------</span>
</span><span id="generate_dctii_m_transform_pair-173"><a href="#generate_dctii_m_transform_pair-173"><span class="linenos">173</span></a><span class="sd">    M : Callable[[ndarray], ndarray]</span>
</span><span id="generate_dctii_m_transform_pair-174"><a href="#generate_dctii_m_transform_pair-174"><span class="linenos">174</span></a><span class="sd">        A function which expects an order-3 tensor as input, and applies the</span>
</span><span id="generate_dctii_m_transform_pair-175"><a href="#generate_dctii_m_transform_pair-175"><span class="linenos">175</span></a><span class="sd">        DCT-II transorm to each of the tubal fibres. This preserves the</span>
</span><span id="generate_dctii_m_transform_pair-176"><a href="#generate_dctii_m_transform_pair-176"><span class="linenos">176</span></a><span class="sd">        dimensions of the tensor.</span>
</span><span id="generate_dctii_m_transform_pair-177"><a href="#generate_dctii_m_transform_pair-177"><span class="linenos">177</span></a>
</span><span id="generate_dctii_m_transform_pair-178"><a href="#generate_dctii_m_transform_pair-178"><span class="linenos">178</span></a><span class="sd">    Minv : Callable[[ndarray], ndarray]</span>
</span><span id="generate_dctii_m_transform_pair-179"><a href="#generate_dctii_m_transform_pair-179"><span class="linenos">179</span></a><span class="sd">        A function implementing the inverse transform of `M`.</span>
</span><span id="generate_dctii_m_transform_pair-180"><a href="#generate_dctii_m_transform_pair-180"><span class="linenos">180</span></a>
</span><span id="generate_dctii_m_transform_pair-181"><a href="#generate_dctii_m_transform_pair-181"><span class="linenos">181</span></a><span class="sd">    References</span>
</span><span id="generate_dctii_m_transform_pair-182"><a href="#generate_dctii_m_transform_pair-182"><span class="linenos">182</span></a><span class="sd">    ----------</span>
</span><span id="generate_dctii_m_transform_pair-183"><a href="#generate_dctii_m_transform_pair-183"><span class="linenos">183</span></a><span class="sd">    Mor, U., Cohen, Y., Valdés-Mas, R., Kviatcovsky, D., Elinav, E. and Avron,</span>
</span><span id="generate_dctii_m_transform_pair-184"><a href="#generate_dctii_m_transform_pair-184"><span class="linenos">184</span></a><span class="sd">    H., 2022. Dimensionality reduction of longitudinal’omics data using modern</span>
</span><span id="generate_dctii_m_transform_pair-185"><a href="#generate_dctii_m_transform_pair-185"><span class="linenos">185</span></a><span class="sd">    tensor factorizations. PLoS Computational Biology, 18(7), p.e1010212.</span>
</span><span id="generate_dctii_m_transform_pair-186"><a href="#generate_dctii_m_transform_pair-186"><span class="linenos">186</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="generate_dctii_m_transform_pair-187"><a href="#generate_dctii_m_transform_pair-187"><span class="linenos">187</span></a>
</span><span id="generate_dctii_m_transform_pair-188"><a href="#generate_dctii_m_transform_pair-188"><span class="linenos">188</span></a>    <span class="k">def</span> <span class="nf">M</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
</span><span id="generate_dctii_m_transform_pair-189"><a href="#generate_dctii_m_transform_pair-189"><span class="linenos">189</span></a>        <span class="n">_assert_t_and_order</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span><span id="generate_dctii_m_transform_pair-190"><a href="#generate_dctii_m_transform_pair-190"><span class="linenos">190</span></a>        <span class="k">return</span> <span class="n">dct</span><span class="p">(</span>
</span><span id="generate_dctii_m_transform_pair-191"><a href="#generate_dctii_m_transform_pair-191"><span class="linenos">191</span></a>            <span class="n">X</span><span class="p">,</span>
</span><span id="generate_dctii_m_transform_pair-192"><a href="#generate_dctii_m_transform_pair-192"><span class="linenos">192</span></a>            <span class="nb">type</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
</span><span id="generate_dctii_m_transform_pair-193"><a href="#generate_dctii_m_transform_pair-193"><span class="linenos">193</span></a>            <span class="n">n</span><span class="o">=</span><span class="n">t</span><span class="p">,</span>
</span><span id="generate_dctii_m_transform_pair-194"><a href="#generate_dctii_m_transform_pair-194"><span class="linenos">194</span></a>            <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
</span><span id="generate_dctii_m_transform_pair-195"><a href="#generate_dctii_m_transform_pair-195"><span class="linenos">195</span></a>            <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">,</span>
</span><span id="generate_dctii_m_transform_pair-196"><a href="#generate_dctii_m_transform_pair-196"><span class="linenos">196</span></a>            <span class="n">workers</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">os</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">(),</span>
</span><span id="generate_dctii_m_transform_pair-197"><a href="#generate_dctii_m_transform_pair-197"><span class="linenos">197</span></a>            <span class="n">overwrite_x</span><span class="o">=</span><span class="n">inplace</span><span class="p">,</span>
</span><span id="generate_dctii_m_transform_pair-198"><a href="#generate_dctii_m_transform_pair-198"><span class="linenos">198</span></a>        <span class="p">)</span>
</span><span id="generate_dctii_m_transform_pair-199"><a href="#generate_dctii_m_transform_pair-199"><span class="linenos">199</span></a>
</span><span id="generate_dctii_m_transform_pair-200"><a href="#generate_dctii_m_transform_pair-200"><span class="linenos">200</span></a>    <span class="k">def</span> <span class="nf">Minv</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
</span><span id="generate_dctii_m_transform_pair-201"><a href="#generate_dctii_m_transform_pair-201"><span class="linenos">201</span></a>        <span class="n">_assert_t_and_order</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span><span id="generate_dctii_m_transform_pair-202"><a href="#generate_dctii_m_transform_pair-202"><span class="linenos">202</span></a>        <span class="k">return</span> <span class="n">idct</span><span class="p">(</span>
</span><span id="generate_dctii_m_transform_pair-203"><a href="#generate_dctii_m_transform_pair-203"><span class="linenos">203</span></a>            <span class="n">X</span><span class="p">,</span>
</span><span id="generate_dctii_m_transform_pair-204"><a href="#generate_dctii_m_transform_pair-204"><span class="linenos">204</span></a>            <span class="nb">type</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
</span><span id="generate_dctii_m_transform_pair-205"><a href="#generate_dctii_m_transform_pair-205"><span class="linenos">205</span></a>            <span class="n">n</span><span class="o">=</span><span class="n">t</span><span class="p">,</span>
</span><span id="generate_dctii_m_transform_pair-206"><a href="#generate_dctii_m_transform_pair-206"><span class="linenos">206</span></a>            <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
</span><span id="generate_dctii_m_transform_pair-207"><a href="#generate_dctii_m_transform_pair-207"><span class="linenos">207</span></a>            <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">,</span>
</span><span id="generate_dctii_m_transform_pair-208"><a href="#generate_dctii_m_transform_pair-208"><span class="linenos">208</span></a>            <span class="n">workers</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">os</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">(),</span>
</span><span id="generate_dctii_m_transform_pair-209"><a href="#generate_dctii_m_transform_pair-209"><span class="linenos">209</span></a>            <span class="n">overwrite_x</span><span class="o">=</span><span class="n">inplace</span><span class="p">,</span>
</span><span id="generate_dctii_m_transform_pair-210"><a href="#generate_dctii_m_transform_pair-210"><span class="linenos">210</span></a>        <span class="p">)</span>
</span><span id="generate_dctii_m_transform_pair-211"><a href="#generate_dctii_m_transform_pair-211"><span class="linenos">211</span></a>
</span><span id="generate_dctii_m_transform_pair-212"><a href="#generate_dctii_m_transform_pair-212"><span class="linenos">212</span></a>    <span class="k">return</span> <span class="n">M</span><span class="p">,</span> <span class="n">Minv</span>
</span></pre></div>


            <div class="docstring"><p>Return <code>M, Minv</code> as defined by the Discrete Cosine Transform of
length <code>t</code>.</p>

<p>Bascially a wrapper around scipy.fft to generate functions to perform
fourier transform based $\times_3 M$ operations, as used by
Mor et al. (2022)</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>t</strong> (int):
The length of the transform</li>
<li><strong>inplace</strong> (bool, default=False):
Control whether or not the generated functions modify the input tensor
in-place, or return a copy with the m-transform applied</li>
<li><strong>norm</strong> ({“backward”, “ortho”, “forward”}, default="ortho"):
See <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dct.html#scipy.fft.dct">https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dct.html#scipy.fft.dct</a></li>
</ul>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>M</strong> (Callable[[ndarray], ndarray]):
A function which expects an order-3 tensor as input, and applies the
DCT-II transorm to each of the tubal fibres. This preserves the
dimensions of the tensor.</li>
<li><strong>Minv</strong> (Callable[[ndarray], ndarray]):
A function implementing the inverse transform of <code>M</code>.</li>
</ul>

<h6 id="references">References</h6>

<p>Mor, U., Cohen, Y., Valdés-Mas, R., Kviatcovsky, D., Elinav, E. and Avron,
H., 2022. Dimensionality reduction of longitudinal’omics data using modern
tensor factorizations. PLoS Computational Biology, 18(7), p.e1010212.</p>
</div>


                </section>
                <section id="generate_dstii_m_transform_pair">
                            <input id="generate_dstii_m_transform_pair-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">generate_dstii_m_transform_pair</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">t</span>, </span><span class="param"><span class="o">*</span>, </span><span class="param"><span class="n">inplace</span><span class="o">=</span><span class="kc">False</span>, </span><span class="param"><span class="n">norm</span><span class="o">=</span><span class="s1">&#39;ortho&#39;</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="generate_dstii_m_transform_pair-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#generate_dstii_m_transform_pair"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="generate_dstii_m_transform_pair-215"><a href="#generate_dstii_m_transform_pair-215"><span class="linenos">215</span></a><span class="k">def</span> <span class="nf">generate_dstii_m_transform_pair</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s2">&quot;ortho&quot;</span><span class="p">):</span>
</span><span id="generate_dstii_m_transform_pair-216"><a href="#generate_dstii_m_transform_pair-216"><span class="linenos">216</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Return `M, Minv` as defined by the Discrete Sine Transform of</span>
</span><span id="generate_dstii_m_transform_pair-217"><a href="#generate_dstii_m_transform_pair-217"><span class="linenos">217</span></a><span class="sd">    length `t`.</span>
</span><span id="generate_dstii_m_transform_pair-218"><a href="#generate_dstii_m_transform_pair-218"><span class="linenos">218</span></a>
</span><span id="generate_dstii_m_transform_pair-219"><a href="#generate_dstii_m_transform_pair-219"><span class="linenos">219</span></a><span class="sd">    Bascially a wrapper around scipy.fft to generate functions to perform</span>
</span><span id="generate_dstii_m_transform_pair-220"><a href="#generate_dstii_m_transform_pair-220"><span class="linenos">220</span></a><span class="sd">    fourier transform based $\\times_3 M$ operations.</span>
</span><span id="generate_dstii_m_transform_pair-221"><a href="#generate_dstii_m_transform_pair-221"><span class="linenos">221</span></a>
</span><span id="generate_dstii_m_transform_pair-222"><a href="#generate_dstii_m_transform_pair-222"><span class="linenos">222</span></a><span class="sd">    Parameters</span>
</span><span id="generate_dstii_m_transform_pair-223"><a href="#generate_dstii_m_transform_pair-223"><span class="linenos">223</span></a><span class="sd">    ----------</span>
</span><span id="generate_dstii_m_transform_pair-224"><a href="#generate_dstii_m_transform_pair-224"><span class="linenos">224</span></a><span class="sd">    t : int</span>
</span><span id="generate_dstii_m_transform_pair-225"><a href="#generate_dstii_m_transform_pair-225"><span class="linenos">225</span></a><span class="sd">        The length of the transform</span>
</span><span id="generate_dstii_m_transform_pair-226"><a href="#generate_dstii_m_transform_pair-226"><span class="linenos">226</span></a>
</span><span id="generate_dstii_m_transform_pair-227"><a href="#generate_dstii_m_transform_pair-227"><span class="linenos">227</span></a><span class="sd">    inplace : bool, default=False</span>
</span><span id="generate_dstii_m_transform_pair-228"><a href="#generate_dstii_m_transform_pair-228"><span class="linenos">228</span></a><span class="sd">        Control whether or not the generated functions modify the input tensor</span>
</span><span id="generate_dstii_m_transform_pair-229"><a href="#generate_dstii_m_transform_pair-229"><span class="linenos">229</span></a><span class="sd">        in-place, or return a copy with the m-transform applied</span>
</span><span id="generate_dstii_m_transform_pair-230"><a href="#generate_dstii_m_transform_pair-230"><span class="linenos">230</span></a>
</span><span id="generate_dstii_m_transform_pair-231"><a href="#generate_dstii_m_transform_pair-231"><span class="linenos">231</span></a><span class="sd">    norm : {“backward”, “ortho”, “forward”}, default=&quot;ortho&quot;</span>
</span><span id="generate_dstii_m_transform_pair-232"><a href="#generate_dstii_m_transform_pair-232"><span class="linenos">232</span></a><span class="sd">        See https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dst.html#scipy.fft.dst</span>
</span><span id="generate_dstii_m_transform_pair-233"><a href="#generate_dstii_m_transform_pair-233"><span class="linenos">233</span></a>
</span><span id="generate_dstii_m_transform_pair-234"><a href="#generate_dstii_m_transform_pair-234"><span class="linenos">234</span></a><span class="sd">    Returns</span>
</span><span id="generate_dstii_m_transform_pair-235"><a href="#generate_dstii_m_transform_pair-235"><span class="linenos">235</span></a><span class="sd">    -------</span>
</span><span id="generate_dstii_m_transform_pair-236"><a href="#generate_dstii_m_transform_pair-236"><span class="linenos">236</span></a><span class="sd">    M : Callable[[ndarray], ndarray]</span>
</span><span id="generate_dstii_m_transform_pair-237"><a href="#generate_dstii_m_transform_pair-237"><span class="linenos">237</span></a><span class="sd">        A function which expects an order-3 tensor as input, and applies the</span>
</span><span id="generate_dstii_m_transform_pair-238"><a href="#generate_dstii_m_transform_pair-238"><span class="linenos">238</span></a><span class="sd">        DST-II transorm to each of the tubal fibres. This preserves the</span>
</span><span id="generate_dstii_m_transform_pair-239"><a href="#generate_dstii_m_transform_pair-239"><span class="linenos">239</span></a><span class="sd">        dimensions of the tensor.</span>
</span><span id="generate_dstii_m_transform_pair-240"><a href="#generate_dstii_m_transform_pair-240"><span class="linenos">240</span></a>
</span><span id="generate_dstii_m_transform_pair-241"><a href="#generate_dstii_m_transform_pair-241"><span class="linenos">241</span></a><span class="sd">    Minv : Callable[[ndarray], ndarray]</span>
</span><span id="generate_dstii_m_transform_pair-242"><a href="#generate_dstii_m_transform_pair-242"><span class="linenos">242</span></a><span class="sd">        A function implementing the inverse transform of `M`.</span>
</span><span id="generate_dstii_m_transform_pair-243"><a href="#generate_dstii_m_transform_pair-243"><span class="linenos">243</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="generate_dstii_m_transform_pair-244"><a href="#generate_dstii_m_transform_pair-244"><span class="linenos">244</span></a>
</span><span id="generate_dstii_m_transform_pair-245"><a href="#generate_dstii_m_transform_pair-245"><span class="linenos">245</span></a>    <span class="k">def</span> <span class="nf">M</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
</span><span id="generate_dstii_m_transform_pair-246"><a href="#generate_dstii_m_transform_pair-246"><span class="linenos">246</span></a>        <span class="n">_assert_t_and_order</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span><span id="generate_dstii_m_transform_pair-247"><a href="#generate_dstii_m_transform_pair-247"><span class="linenos">247</span></a>        <span class="k">return</span> <span class="n">dst</span><span class="p">(</span>
</span><span id="generate_dstii_m_transform_pair-248"><a href="#generate_dstii_m_transform_pair-248"><span class="linenos">248</span></a>            <span class="n">X</span><span class="p">,</span>
</span><span id="generate_dstii_m_transform_pair-249"><a href="#generate_dstii_m_transform_pair-249"><span class="linenos">249</span></a>            <span class="nb">type</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
</span><span id="generate_dstii_m_transform_pair-250"><a href="#generate_dstii_m_transform_pair-250"><span class="linenos">250</span></a>            <span class="n">n</span><span class="o">=</span><span class="n">t</span><span class="p">,</span>
</span><span id="generate_dstii_m_transform_pair-251"><a href="#generate_dstii_m_transform_pair-251"><span class="linenos">251</span></a>            <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
</span><span id="generate_dstii_m_transform_pair-252"><a href="#generate_dstii_m_transform_pair-252"><span class="linenos">252</span></a>            <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">,</span>
</span><span id="generate_dstii_m_transform_pair-253"><a href="#generate_dstii_m_transform_pair-253"><span class="linenos">253</span></a>            <span class="n">workers</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">os</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">(),</span>
</span><span id="generate_dstii_m_transform_pair-254"><a href="#generate_dstii_m_transform_pair-254"><span class="linenos">254</span></a>            <span class="n">overwrite_x</span><span class="o">=</span><span class="n">inplace</span><span class="p">,</span>
</span><span id="generate_dstii_m_transform_pair-255"><a href="#generate_dstii_m_transform_pair-255"><span class="linenos">255</span></a>        <span class="p">)</span>
</span><span id="generate_dstii_m_transform_pair-256"><a href="#generate_dstii_m_transform_pair-256"><span class="linenos">256</span></a>
</span><span id="generate_dstii_m_transform_pair-257"><a href="#generate_dstii_m_transform_pair-257"><span class="linenos">257</span></a>    <span class="k">def</span> <span class="nf">Minv</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
</span><span id="generate_dstii_m_transform_pair-258"><a href="#generate_dstii_m_transform_pair-258"><span class="linenos">258</span></a>        <span class="n">_assert_t_and_order</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span><span id="generate_dstii_m_transform_pair-259"><a href="#generate_dstii_m_transform_pair-259"><span class="linenos">259</span></a>        <span class="k">return</span> <span class="n">idst</span><span class="p">(</span>
</span><span id="generate_dstii_m_transform_pair-260"><a href="#generate_dstii_m_transform_pair-260"><span class="linenos">260</span></a>            <span class="n">X</span><span class="p">,</span>
</span><span id="generate_dstii_m_transform_pair-261"><a href="#generate_dstii_m_transform_pair-261"><span class="linenos">261</span></a>            <span class="nb">type</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
</span><span id="generate_dstii_m_transform_pair-262"><a href="#generate_dstii_m_transform_pair-262"><span class="linenos">262</span></a>            <span class="n">n</span><span class="o">=</span><span class="n">t</span><span class="p">,</span>
</span><span id="generate_dstii_m_transform_pair-263"><a href="#generate_dstii_m_transform_pair-263"><span class="linenos">263</span></a>            <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
</span><span id="generate_dstii_m_transform_pair-264"><a href="#generate_dstii_m_transform_pair-264"><span class="linenos">264</span></a>            <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">,</span>
</span><span id="generate_dstii_m_transform_pair-265"><a href="#generate_dstii_m_transform_pair-265"><span class="linenos">265</span></a>            <span class="n">workers</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">os</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">(),</span>
</span><span id="generate_dstii_m_transform_pair-266"><a href="#generate_dstii_m_transform_pair-266"><span class="linenos">266</span></a>            <span class="n">overwrite_x</span><span class="o">=</span><span class="n">inplace</span><span class="p">,</span>
</span><span id="generate_dstii_m_transform_pair-267"><a href="#generate_dstii_m_transform_pair-267"><span class="linenos">267</span></a>        <span class="p">)</span>
</span><span id="generate_dstii_m_transform_pair-268"><a href="#generate_dstii_m_transform_pair-268"><span class="linenos">268</span></a>
</span><span id="generate_dstii_m_transform_pair-269"><a href="#generate_dstii_m_transform_pair-269"><span class="linenos">269</span></a>    <span class="k">return</span> <span class="n">M</span><span class="p">,</span> <span class="n">Minv</span>
</span></pre></div>


            <div class="docstring"><p>Return <code>M, Minv</code> as defined by the Discrete Sine Transform of
length <code>t</code>.</p>

<p>Bascially a wrapper around scipy.fft to generate functions to perform
fourier transform based $\times_3 M$ operations.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>t</strong> (int):
The length of the transform</li>
<li><strong>inplace</strong> (bool, default=False):
Control whether or not the generated functions modify the input tensor
in-place, or return a copy with the m-transform applied</li>
<li><strong>norm</strong> ({“backward”, “ortho”, “forward”}, default="ortho"):
See <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dst.html#scipy.fft.dst">https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dst.html#scipy.fft.dst</a></li>
</ul>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>M</strong> (Callable[[ndarray], ndarray]):
A function which expects an order-3 tensor as input, and applies the
DST-II transorm to each of the tubal fibres. This preserves the
dimensions of the tensor.</li>
<li><strong>Minv</strong> (Callable[[ndarray], ndarray]):
A function implementing the inverse transform of <code>M</code>.</li>
</ul>
</div>


                </section>
                <section id="facewise_product">
                            <input id="facewise_product-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">facewise_product</span><span class="signature pdoc-code condensed">(<span class="param"><span class="o">*</span><span class="n">tensors</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="facewise_product-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#facewise_product"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="facewise_product-19"><a href="#facewise_product-19"><span class="linenos">19</span></a><span class="k">def</span> <span class="nf">facewise_product</span><span class="p">(</span><span class="o">*</span><span class="n">tensors</span><span class="p">):</span>
</span><span id="facewise_product-20"><a href="#facewise_product-20"><span class="linenos">20</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute cumulative facewise product.</span>
</span><span id="facewise_product-21"><a href="#facewise_product-21"><span class="linenos">21</span></a>
</span><span id="facewise_product-22"><a href="#facewise_product-22"><span class="linenos">22</span></a><span class="sd">    Definition:</span>
</span><span id="facewise_product-23"><a href="#facewise_product-23"><span class="linenos">23</span></a><span class="sd">    $$</span>
</span><span id="facewise_product-24"><a href="#facewise_product-24"><span class="linenos">24</span></a><span class="sd">        C_{:,:,i} = A_{:,:,i} B_{:,:,i}</span>
</span><span id="facewise_product-25"><a href="#facewise_product-25"><span class="linenos">25</span></a><span class="sd">    $$</span>
</span><span id="facewise_product-26"><a href="#facewise_product-26"><span class="linenos">26</span></a>
</span><span id="facewise_product-27"><a href="#facewise_product-27"><span class="linenos">27</span></a><span class="sd">    Parameters</span>
</span><span id="facewise_product-28"><a href="#facewise_product-28"><span class="linenos">28</span></a><span class="sd">    ----------</span>
</span><span id="facewise_product-29"><a href="#facewise_product-29"><span class="linenos">29</span></a><span class="sd">    *tensors : ndarray</span>
</span><span id="facewise_product-30"><a href="#facewise_product-30"><span class="linenos">30</span></a><span class="sd">        Variable number of tensors, such that all adjacent input tensors have</span>
</span><span id="facewise_product-31"><a href="#facewise_product-31"><span class="linenos">31</span></a><span class="sd">        shape (a, b, d) and shape (b, c, d) respectively</span>
</span><span id="facewise_product-32"><a href="#facewise_product-32"><span class="linenos">32</span></a>
</span><span id="facewise_product-33"><a href="#facewise_product-33"><span class="linenos">33</span></a><span class="sd">    Returns</span>
</span><span id="facewise_product-34"><a href="#facewise_product-34"><span class="linenos">34</span></a><span class="sd">    -------</span>
</span><span id="facewise_product-35"><a href="#facewise_product-35"><span class="linenos">35</span></a><span class="sd">    C : ndarray, shape: (a, c, d)</span>
</span><span id="facewise_product-36"><a href="#facewise_product-36"><span class="linenos">36</span></a><span class="sd">        Facewise tensor product</span>
</span><span id="facewise_product-37"><a href="#facewise_product-37"><span class="linenos">37</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="facewise_product-38"><a href="#facewise_product-38"><span class="linenos">38</span></a>    <span class="c1"># apply the lambda function cumulatively over the tensor inputs</span>
</span><span id="facewise_product-39"><a href="#facewise_product-39"><span class="linenos">39</span></a>    <span class="k">return</span> <span class="n">reduce</span><span class="p">(</span><span class="n">_BINARY_FACEWISE</span><span class="p">,</span> <span class="n">tensors</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Compute cumulative facewise product.</p>

<p>Definition:
$$
    C_{:,:,i} = A_{:,:,i} B_{:,:,i}
$$</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>*tensors</strong> (ndarray):
Variable number of tensors, such that all adjacent input tensors have
shape (a, b, d) and shape (b, c, d) respectively</li>
</ul>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>C : ndarray, shape</strong> ((a, c, d)):
Facewise tensor product</li>
</ul>
</div>


                </section>
                <section id="m_product">
                            <input id="m_product-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">m_product</span><span class="signature pdoc-code condensed">(<span class="param"><span class="o">*</span><span class="n">tensors</span>, </span><span class="param"><span class="o">**</span><span class="n">transforms</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="m_product-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#m_product"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="m_product-42"><a href="#m_product-42"><span class="linenos">42</span></a><span class="k">def</span> <span class="nf">m_product</span><span class="p">(</span><span class="o">*</span><span class="n">tensors</span><span class="p">,</span> <span class="o">**</span><span class="n">transforms</span><span class="p">):</span>
</span><span id="m_product-43"><a href="#m_product-43"><span class="linenos">43</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Kilmer et al. (2021) tensor m-product for order-3 tensors.</span>
</span><span id="m_product-44"><a href="#m_product-44"><span class="linenos">44</span></a>
</span><span id="m_product-45"><a href="#m_product-45"><span class="linenos">45</span></a><span class="sd">    See paper for definition.</span>
</span><span id="m_product-46"><a href="#m_product-46"><span class="linenos">46</span></a>
</span><span id="m_product-47"><a href="#m_product-47"><span class="linenos">47</span></a><span class="sd">    Parameters</span>
</span><span id="m_product-48"><a href="#m_product-48"><span class="linenos">48</span></a><span class="sd">    ----------</span>
</span><span id="m_product-49"><a href="#m_product-49"><span class="linenos">49</span></a><span class="sd">    *tensors : ndarray</span>
</span><span id="m_product-50"><a href="#m_product-50"><span class="linenos">50</span></a><span class="sd">        Variable number of tensors, such that all adjacent input tensors have</span>
</span><span id="m_product-51"><a href="#m_product-51"><span class="linenos">51</span></a><span class="sd">        shape (a, b, d) and shape (b, c, d) respectively</span>
</span><span id="m_product-52"><a href="#m_product-52"><span class="linenos">52</span></a>
</span><span id="m_product-53"><a href="#m_product-53"><span class="linenos">53</span></a><span class="sd">    M : Callable[[ndarray], ndarray] or None, default=None</span>
</span><span id="m_product-54"><a href="#m_product-54"><span class="linenos">54</span></a><span class="sd">        A function which, given some order-3 tensor, returns it under an</span>
</span><span id="m_product-55"><a href="#m_product-55"><span class="linenos">55</span></a><span class="sd">        orthogonal tubal transformation</span>
</span><span id="m_product-56"><a href="#m_product-56"><span class="linenos">56</span></a>
</span><span id="m_product-57"><a href="#m_product-57"><span class="linenos">57</span></a><span class="sd">    MInv : Callable[[ndarray], ndarray] or None, default=None</span>
</span><span id="m_product-58"><a href="#m_product-58"><span class="linenos">58</span></a><span class="sd">        A function implementing the inverse tubal transformation of M</span>
</span><span id="m_product-59"><a href="#m_product-59"><span class="linenos">59</span></a>
</span><span id="m_product-60"><a href="#m_product-60"><span class="linenos">60</span></a><span class="sd">    Returns</span>
</span><span id="m_product-61"><a href="#m_product-61"><span class="linenos">61</span></a><span class="sd">    -------</span>
</span><span id="m_product-62"><a href="#m_product-62"><span class="linenos">62</span></a><span class="sd">    m_product : ndarray, shape: (a, c, d)</span>
</span><span id="m_product-63"><a href="#m_product-63"><span class="linenos">63</span></a><span class="sd">        Tensor-tensor m-product as found in Kilmer et al. (2021)</span>
</span><span id="m_product-64"><a href="#m_product-64"><span class="linenos">64</span></a>
</span><span id="m_product-65"><a href="#m_product-65"><span class="linenos">65</span></a><span class="sd">    References</span>
</span><span id="m_product-66"><a href="#m_product-66"><span class="linenos">66</span></a><span class="sd">    ----------</span>
</span><span id="m_product-67"><a href="#m_product-67"><span class="linenos">67</span></a><span class="sd">    Kilmer, M.E., Horesh, L., Avron, H. and Newman, E., 2021. Tensor-tensor</span>
</span><span id="m_product-68"><a href="#m_product-68"><span class="linenos">68</span></a><span class="sd">    algebra for optimal representation and compression of multiway data.</span>
</span><span id="m_product-69"><a href="#m_product-69"><span class="linenos">69</span></a><span class="sd">    Proceedings of the National Academy of Sciences, 118(28), p.e2015851118.</span>
</span><span id="m_product-70"><a href="#m_product-70"><span class="linenos">70</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="m_product-71"><a href="#m_product-71"><span class="linenos">71</span></a>    <span class="c1"># process the m-transform keyword inputs, applying defaults if needed</span>
</span><span id="m_product-72"><a href="#m_product-72"><span class="linenos">72</span></a>    <span class="n">default_transforms</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;M&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Minv&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
</span><span id="m_product-73"><a href="#m_product-73"><span class="linenos">73</span></a>    <span class="n">transforms</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">default_transforms</span><span class="p">,</span> <span class="o">**</span><span class="n">transforms</span><span class="p">}</span>
</span><span id="m_product-74"><a href="#m_product-74"><span class="linenos">74</span></a>    <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span>
</span><span id="m_product-75"><a href="#m_product-75"><span class="linenos">75</span></a>        <span class="nb">callable</span><span class="p">(</span><span class="n">transforms</span><span class="p">[</span><span class="s2">&quot;M&quot;</span><span class="p">])</span> <span class="o">^</span> <span class="nb">callable</span><span class="p">(</span><span class="n">transforms</span><span class="p">[</span><span class="s2">&quot;Minv&quot;</span><span class="p">])</span>
</span><span id="m_product-76"><a href="#m_product-76"><span class="linenos">76</span></a>    <span class="p">),</span> <span class="s2">&quot;If explicitly defined, both M and its inverse must be defined&quot;</span>
</span><span id="m_product-77"><a href="#m_product-77"><span class="linenos">77</span></a>
</span><span id="m_product-78"><a href="#m_product-78"><span class="linenos">78</span></a>    <span class="k">if</span> <span class="ow">not</span> <span class="nb">callable</span><span class="p">(</span><span class="n">transforms</span><span class="p">[</span><span class="s2">&quot;M&quot;</span><span class="p">]):</span>
</span><span id="m_product-79"><a href="#m_product-79"><span class="linenos">79</span></a>        <span class="n">M</span><span class="p">,</span> <span class="n">Minv</span> <span class="o">=</span> <span class="n">generate_default_m_transform_pair</span><span class="p">(</span><span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span><span id="m_product-80"><a href="#m_product-80"><span class="linenos">80</span></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="m_product-81"><a href="#m_product-81"><span class="linenos">81</span></a>        <span class="n">M</span><span class="p">,</span> <span class="n">Minv</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">[</span><span class="s2">&quot;M&quot;</span><span class="p">],</span> <span class="n">transforms</span><span class="p">[</span><span class="s2">&quot;Minv&quot;</span><span class="p">]</span>
</span><span id="m_product-82"><a href="#m_product-82"><span class="linenos">82</span></a>
</span><span id="m_product-83"><a href="#m_product-83"><span class="linenos">83</span></a>    <span class="c1"># use a generator representing all the tensors under the m-transform</span>
</span><span id="m_product-84"><a href="#m_product-84"><span class="linenos">84</span></a>    <span class="c1"># (&#39;hat space&#39;), allowing it to be lazily reduced over using the binary</span>
</span><span id="m_product-85"><a href="#m_product-85"><span class="linenos">85</span></a>    <span class="c1"># facewise product anonymous function. prevents storing all of the</span>
</span><span id="m_product-86"><a href="#m_product-86"><span class="linenos">86</span></a>    <span class="c1"># transformed tensors in memory at the same time</span>
</span><span id="m_product-87"><a href="#m_product-87"><span class="linenos">87</span></a>    <span class="k">return</span> <span class="n">Minv</span><span class="p">(</span><span class="n">reduce</span><span class="p">(</span><span class="n">_BINARY_FACEWISE</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">(</span><span class="n">tens</span><span class="p">)</span> <span class="k">for</span> <span class="n">tens</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">)))</span>
</span></pre></div>


            <div class="docstring"><p>Kilmer et al. (2021) tensor m-product for order-3 tensors.</p>

<p>See paper for definition.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>*tensors</strong> (ndarray):
Variable number of tensors, such that all adjacent input tensors have
shape (a, b, d) and shape (b, c, d) respectively</li>
<li><strong>M</strong> (Callable[[ndarray], ndarray] or None, default=None):
A function which, given some order-3 tensor, returns it under an
orthogonal tubal transformation</li>
<li><strong>MInv</strong> (Callable[[ndarray], ndarray] or None, default=None):
A function implementing the inverse tubal transformation of M</li>
</ul>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>m_product : ndarray, shape</strong> ((a, c, d)):
Tensor-tensor m-product as found in Kilmer et al. (2021)</li>
</ul>

<h6 id="references">References</h6>

<p>Kilmer, M.E., Horesh, L., Avron, H. and Newman, E., 2021. Tensor-tensor
algebra for optimal representation and compression of multiway data.
Proceedings of the National Academy of Sciences, 118(28), p.e2015851118.</p>
</div>


                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>